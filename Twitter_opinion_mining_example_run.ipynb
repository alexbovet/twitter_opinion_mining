{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Opinion Mining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'ds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b495a43f7b3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0maddStatSigniHT\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maddStatSigniHT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mselectHashtags\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mselectHashtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mupdateHTGroups\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mupdateHTGroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbuildTrainingSet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuildTrainingSet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcrossValOptimize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcrossValOptimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/alex/network_workdir/twitter_opinion_mining/updateHTGroups.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mTwSqliteDB\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maddHTSupportGroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mupdateHTGroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'ds'"
     ]
    }
   ],
   "source": [
    "# import the different modules\n",
    "from buildDatabase import buildDatabse\n",
    "from makeHTnetwork import makeHTNetwork\n",
    "from selectInitialHashtags import selectInitialHashtags\n",
    "from propagateLabels import propagateLabels\n",
    "from addStatSigniHT import addStatSigniHT\n",
    "from selectHashtags import selectHashtags\n",
    "from updateHTGroups import updateHTGroups\n",
    "from buildTrainingSet import buildTrainingSet\n",
    "from crossValOptimize import crossValOptimize\n",
    "from trainClassifier import trainClassifier\n",
    "from classifyTweets import classifyTweets\n",
    "from makeProbaDF import makeProbaDF\n",
    "from analyzeProbaDF import analyzeProbaDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define filenames and directories for current job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of directories containing the tweet archive files (TAJ)\n",
    "tweet_archive_dirs = ['etrade']\n",
    "\n",
    "# SQLite database that will be created\n",
    "sqlite_file = 'test.sqlite'\n",
    "\n",
    "# hashtag co-occurrence graph that will be created\n",
    "graph_file = 'graph_file.graphml'\n",
    "\n",
    "# pickle files where the training set features will be saved\n",
    "features_pickle_file = 'features.pickle'\n",
    "\n",
    "# pickle file where the training set labels will be saved\n",
    "labels_pickle_file = 'labels.pickle'\n",
    "\n",
    "# vectorized features file\n",
    "features_vect_file = 'features.mmap'\n",
    "\n",
    "# vectorized labels file\n",
    "labels_vect_file = 'labels.mmap'\n",
    "\n",
    "# mapping between labels names and numbers\n",
    "labels_mappers_file = 'labels_mappers.pickle'\n",
    "\n",
    "# JSON file with the classifier best parameters obtained from cross-validation\n",
    "best_params_file = 'best_params.json'\n",
    "\n",
    "# where the trained calssifier will be saved\n",
    "classifier_filename = 'classifier.pickle'\n",
    "\n",
    "# DataFrame with the results of the label propagation\n",
    "# on the hashtag network\n",
    "propag_results_filename = 'propag_results.pickle'\n",
    "\n",
    "# DataFrame with the classification probability of\n",
    "# every tweets in the database\n",
    "df_proba_filename = 'df_proba.pickle'\n",
    "\n",
    "# DataFrame with the number of tweets in each camp per day\n",
    "df_num_tweets_filename = 'df_num_tweets.pickle'\n",
    "\n",
    "# DataFrame with the number of users in each camp per day\n",
    "df_num_users_filename = 'df_num_users.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all the parameters are saved in this dictionary \n",
    "# that will be passed to the differetn modules\n",
    "job = {'tweet_archive_dirs': tweet_archive_dirs,\n",
    "       'sqlite_db_filename' : sqlite_file,\n",
    "       'graph_file' : graph_file,\n",
    "       'propag_results_filename' : propag_results_filename,\n",
    "      'features_pickle_file': features_pickle_file,\n",
    "      'labels_pickle_file': labels_pickle_file,\n",
    "      'features_vect_file': features_vect_file,\n",
    "      'labels_vect_file': labels_vect_file,\n",
    "      'labels_mappers_file' : labels_mappers_file,\n",
    "       'classifier_filename':classifier_filename,\n",
    "       'df_proba_filename':df_proba_filename,\n",
    "       'df_num_tweets_filename': df_num_tweets_filename,\n",
    "       'df_num_users_filename': df_num_users_filename,\n",
    "       'best_params_file' : best_params_file\n",
    "       }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build the SQLite database with the extracted info from the tweets\n",
    "Read the tweets from all the .taj files in the directories `tweet_archive_dirs`\n",
    "and add them to the database `sqlite_db_filename`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping index sqlite_autoindex_hashtag_1\n",
      "index associated with UNIQUE or PRIMARY KEY constraint cannot be dropped\n",
      "Dropping index sqlite_autoindex_hashtag_tweet_user_1\n",
      "index associated with UNIQUE or PRIMARY KEY constraint cannot be dropped\n",
      "Dropping index sqlite_autoindex_tweet_to_keyword_1\n",
      "index associated with UNIQUE or PRIMARY KEY constraint cannot be dropped\n",
      "Dropping index sqlite_autoindex_tweet_to_mentioned_uid_1\n",
      "index associated with UNIQUE or PRIMARY KEY constraint cannot be dropped\n",
      "Dropping index sqlite_autoindex_filename_1\n",
      "index associated with UNIQUE or PRIMARY KEY constraint cannot be dropped\n",
      "Dropping index sqlite_autoindex_query_1\n",
      "index associated with UNIQUE or PRIMARY KEY constraint cannot be dropped\n",
      "Dropping index sqlite_autoindex_source_content_1\n",
      "index associated with UNIQUE or PRIMARY KEY constraint cannot be dropped\n",
      "Dropping index sqlite_autoindex_source_url_1\n",
      "index associated with UNIQUE or PRIMARY KEY constraint cannot be dropped\n",
      "Dropping index sqlite_autoindex_tweet_to_query_id_1\n",
      "index associated with UNIQUE or PRIMARY KEY constraint cannot be dropped\n",
      "Dropping index timestamp_index\n",
      "Dropping index user_index\n",
      "Dropping index retweet_timestamp_index\n",
      "Dropping index retweet_user_index\n",
      "Dropping index ht_count_index\n",
      "Dropping index tweet_id_index\n",
      "Dropping index hashtag_index\n",
      "Dropping index user_id_index\n",
      "Dropping index tweet_id_mention_index\n",
      "Dropping index keyword_index\n",
      "Dropping index retweet_author_index\n",
      "Dropping index mention_author_index\n",
      "Dropping index reply_author_index\n",
      "Dropping index quote_author_index\n",
      "Dropping index tweet_id_retweet_index\n",
      "Dropping index retweet_id_retweet_index\n",
      "Dropping index tweet_id_reply_index\n",
      "Dropping index tweet_id_quote_index\n",
      "Dropping index source_content_tweet_index\n",
      "Dropping index tweet_id_source_content\n",
      "Dropping index tweet_id_query_id_index\n",
      "0 over 1\n",
      "... getting data from etrade/tweets-b15b7e5b-a99f-4612-a24d-c452dbc0b9fb.taj\n",
      "... took 2.574s\n",
      "\n",
      "*** updating sqlite tables...\n",
      "*** took 0.3461s\n",
      "\n",
      "Finished\n",
      "Total time 2.92112s\n",
      "Transaction time 2.93303s\n",
      "Total time 2.93684s\n",
      "sqlite_file : test.sqlite\n",
      "Creating indexes\n",
      "time 0.20323s\n"
     ]
    }
   ],
   "source": [
    "buildDatabse(job).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Make the Hashtag co-occurrences network\n",
    "Reads all the co-occurences from the SQLite database and builds the network\n",
    "of where nodes are hashtags and edges are co-occurrences.\n",
    "The graph is a [*graph-tool*](https://graph-tool.skewed.de/) object and is saved in graphml format to `graph_file`.\n",
    "\n",
    "Nodes of the graph have two properties: `counts` is the number of single occurrences of the hashtag and `name` is the name of the hashtag.\n",
    "\n",
    "Edges have a property `weights` equal to the number of co-occurrences they represent.\n",
    "\n",
    "The graph has the following properties saved with it:\n",
    "- `Ntweets`: number of tweets with at least one hashtag used to build the graph.\n",
    "- `start_date` : date of the first tweet.\n",
    "- `stop_date` : date of the last tweet.\n",
    "- `weight_threshold` : co-occurrence threshold. Edges with less than `weight_threshold` co-occurrences are discarded.\n",
    "\n",
    "*Optional parameters that can be added to `job`:*\n",
    "- `start_date` and `stop_date` to specify a time range for the tweets. (Default is `None`, i.e. select all the tweets in the database).\n",
    "- `weight_threshold` is the minimum number of co-occurences between to hashtag to be included in the graph. (Default is 3).\n",
    "\n",
    "To add a parameter to job, simply execute `job[\"parameter name\"] = parameter value`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating edge list\n",
      "*** took 1.912s\n",
      "creating graph\n",
      "*** took 0.004421s\n"
     ]
    }
   ],
   "source": [
    "makeHTNetwork(job).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add statistical significance value to edges\n",
    "Adds a property `s` to edges of the graph corresponding to the statistical significance (`s = log10(p_0/p)`)\n",
    "of the co-occurence computed from a null model[1].\n",
    "The computation is done using `p0=1e-6` and `p0` is saved as a graph property.\n",
    "Different values of `p0` can be latter tested by shifting `s`.\n",
    "The resulting graph is saved to `graph_file`.\n",
    "\n",
    "*Optional parameters that can be added to `job`:*\n",
    "- `ncpu` : number of processors to be used. (Default is 6).\n",
    "\n",
    "\n",
    "[1] Martinez-Romo, J. et al. Disentangling categorical relationships through a graph of co-occurrences. Phys. Rev. E 84, 1â€“8 (2011).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing significance of links\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=6)]: Done 188 tasks      | elapsed:   13.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "*** took 37.58s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done 351 out of 351 | elapsed:   37.4s finished\n"
     ]
    }
   ],
   "source": [
    "addStatSigniHT(job).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Select the initial hashtags to start the propagation\n",
    "This will display to top occurring hashtags.\n",
    "\n",
    "*Optional parameters that can be added to `job`:*\n",
    "- `num_top_htgs` : (Default is top 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Top 100 occuring hashtags:\n",
      "* rank: (name: frequency)\n",
      "0 :('finance', 8162)\n",
      "1 :('etrade', 8028)\n",
      "2 :('tradeking', 7663)\n",
      "3 :('money', 7436)\n",
      "4 :('stock', 4169)\n",
      "5 :('401k', 4141)\n",
      "6 :('amtd', 4141)\n",
      "7 :('alerts', 4141)\n",
      "8 :('stocks', 3937)\n",
      "9 :('stockmarket', 3915)\n",
      "10 :('cash', 3911)\n",
      "11 :('market', 3523)\n",
      "12 :('ameritrade', 3522)\n",
      "13 :('scottrade', 3522)\n",
      "14 :('mortgage', 714)\n",
      "15 :('rates', 352)\n",
      "16 :('didyouknow', 210)\n",
      "17 :('loan', 94)\n",
      "18 :('loans', 76)\n",
      "19 :('seattle', 70)\n",
      "20 :('history', 70)\n",
      "21 :('interest', 70)\n",
      "22 :('canadian', 68)\n",
      "23 :('motors', 58)\n",
      "24 :('kia', 58)\n",
      "25 :('hiring', 49)\n",
      "26 :('year', 47)\n",
      "27 :('house', 45)\n",
      "28 :('news', 45)\n",
      "29 :('a', 44)\n",
      "30 :('get', 44)\n",
      "31 :('major', 40)\n",
      "32 :('rate', 39)\n",
      "33 :('refinancing', 38)\n",
      "34 :('boycottcnn', 36)\n",
      "35 :('calculator', 35)\n",
      "36 :('lenders', 35)\n",
      "37 :('daytrading', 32)\n",
      "38 :('stocktrader', 31)\n",
      "39 :('trump', 30)\n",
      "40 :('vermont', 30)\n",
      "41 :('2nd', 29)\n",
      "42 :('mortgages', 29)\n",
      "43 :('estimate', 29)\n",
      "44 :('calcu', 27)\n",
      "45 :('ecommerce', 26)\n",
      "46 :('jobsearch', 26)\n",
      "47 :('alaska', 26)\n",
      "48 :('etrade4all', 22)\n",
      "49 :('direct', 22)\n",
      "50 :('nasdaq', 19)\n",
      "51 :('carrington', 19)\n",
      "52 :('services', 19)\n",
      "53 :('blackoutcnn', 19)\n",
      "54 :('amortization', 18)\n",
      "55 :('with', 18)\n",
      "56 :('invest', 18)\n",
      "57 :('homedepot', 18)\n",
      "58 :('indianapolis', 18)\n",
      "59 :('table', 18)\n",
      "60 :('school', 17)\n",
      "61 :('motorola', 17)\n",
      "62 :('usa', 16)\n",
      "63 :('entertainment', 16)\n",
      "64 :('prediction', 16)\n",
      "65 :('investing', 16)\n",
      "66 :('postseason', 16)\n",
      "67 :('air', 16)\n",
      "68 :('jobs', 16)\n",
      "69 :('tax', 16)\n",
      "70 :('jerseycitynj', 15)\n",
      "71 :('job', 15)\n",
      "72 :('jerseycity', 15)\n",
      "73 :('indiana', 14)\n",
      "74 :('fortwayne', 13)\n",
      "75 :('robinhood', 13)\n",
      "76 :('colleges', 13)\n",
      "77 :('subprime', 13)\n",
      "78 :('home', 13)\n",
      "79 :('www', 13)\n",
      "80 :('wallstreet', 12)\n",
      "81 :('bostonma', 12)\n",
      "82 :('jobssearch', 12)\n",
      "83 :('boston', 12)\n",
      "84 :('alterconf', 12)\n",
      "85 :('app', 12)\n",
      "86 :('boycottpepsi', 12)\n",
      "87 :('freedom', 12)\n",
      "88 :('print', 12)\n",
      "89 :('stloius', 12)\n",
      "90 :('predictions', 9)\n",
      "91 :('wellsfargo', 9)\n",
      "92 :('out', 9)\n",
      "93 :('gainers', 9)\n",
      "94 :('losers', 9)\n",
      "95 :('charts', 9)\n",
      "96 :('ilcoin', 8)\n",
      "97 :('geico', 8)\n",
      "98 :('sprint', 8)\n",
      "99 :('seo', 8)\n"
     ]
    }
   ],
   "source": [
    "selectInitialHashtags(job).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select seeds hashtags you want to use from the list (minimum two) \n",
    "and add them to the `job` dictionary with the key `initial_htgs_lists`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initial_htgs_lists is a list of list with hashtags seeds for each camp:\n",
    "job['initial_htgs_lists'] = [['money'],\n",
    "                             ['401k']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Propagate labels to neighboring hashtags\n",
    "This part can be looped by updating the `htgs_lists` in `job` with the result of the label propagation to reach a larger number of hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start with the hashtag seeds selected above.\n",
    "job['htgs_lists'] = job['initial_htgs_lists']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loop has two steps:\n",
    "1. `propagateLabels` uses the graph from `graph_file` and the initial hashtags from `initial_htgs_lists` to propagate their labels to their neighbors taking into account the statistical significance of edges. The results are saved in a pandas DataFrame in `propag_results_filename`.\n",
    "    - *Optional parameters that can be added to `job`:*\n",
    "        - `count_ratio` : threshold, $r$, for removing hashtags with a number of single occurrences smaller than $r \\max\\limits_{v_j\\in C_k} c_j$ where $c_i$ is the number of occurrences of the hashtag associated with vertex $v_i$, $C_k$ is the class to which $v_i$ belong. (Default = 0.001).\n",
    "        - `p0` : significance threshold. to keep only edges with p_val <= p0. (Default = 1e-5).\n",
    "\n",
    "2. Visualisation of the results using `selectHashtags`, and updating the `initial_htgs_lists` list. This will print a list of hashtags, $i$, for each camp $C_k$ satisfying: $\\sum_{j \\in C_k} s_{ij} > \\sum_{j \\in C_l} s_{ij}$, where $C_l$ represents all the other camps than $C_k$.\n",
    "    - *Optional parameters that can be added to `job`:*\n",
    "        - `num_top_htgs` : number of top hashtags to be displayed in each camp. (Default is 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propagating labels\n",
      "saving results\n"
     ]
    }
   ],
   "source": [
    "# 1st step of the loop:\n",
    "propagateLabels(job).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " +++ hashtags in camp 2\n",
      "           name  count  label_init  vertex_id  label_sum2    signi_sum2  \\\n",
      "5       finance   8162           2          5         4.0   4527.106262   \n",
      "0        etrade   8028           2          0         5.0   8079.589318   \n",
      "64    tradeking   7663           2         64         5.0   8517.992058   \n",
      "52        stock   4169           2         52         6.0  14079.147091   \n",
      "60         401k   4141           2         60         6.0  14382.367984   \n",
      "66         amtd   4141           2         66         6.0  14382.367984   \n",
      "78       alerts   4141           2         78         6.0  14382.367984   \n",
      "44       motors     58          -1         44         1.0      7.655783   \n",
      "54          kia     58          -1         54         1.0      7.655783   \n",
      "98        major     40          -1         98         1.0      3.720529   \n",
      "71   daytrading     32          -1         71         1.0      0.846667   \n",
      "72  stocktrader     31          -1         72         1.0      1.978751   \n",
      "83      vermont     30          -1         83         1.0      1.753301   \n",
      "79       alaska     26          -1         79         1.0      0.851720   \n",
      "48    alterconf     12          -1         48         1.0      1.117169   \n",
      "49          app     12          -1         49         1.0      1.117169   \n",
      "\n",
      "    label_sum1   signi_sum1  \n",
      "5          3.0  3095.052858  \n",
      "0          3.0  2898.263628  \n",
      "64         3.0  3178.049834  \n",
      "52         0.0     0.000000  \n",
      "60         0.0     0.000000  \n",
      "66         0.0     0.000000  \n",
      "78         0.0     0.000000  \n",
      "44         0.0     0.000000  \n",
      "54         0.0     0.000000  \n",
      "98         0.0     0.000000  \n",
      "71         0.0     0.000000  \n",
      "72         0.0     0.000000  \n",
      "83         0.0     0.000000  \n",
      "79         0.0     0.000000  \n",
      "48         0.0     0.000000  \n",
      "49         0.0     0.000000  \n",
      "\n",
      " +++ hashtags in camp 1\n",
      "           name  count  label_init  vertex_id  label_sum2   signi_sum2  \\\n",
      "57        money   7436           1         57         0.0     0.000000   \n",
      "28       stocks   3937           1         28         1.0  1000.856308   \n",
      "31  stockmarket   3915           1         31         1.0  1042.037275   \n",
      "70         cash   3911           1         70         1.0  1052.159275   \n",
      "63       market   3523           1         63         2.0  2023.458314   \n",
      "58   ameritrade   3522           1         58         2.0  2026.427574   \n",
      "74    scottrade   3522           1         74         2.0  2026.427574   \n",
      "\n",
      "    label_sum1   signi_sum1  \n",
      "57         6.0  7115.299230  \n",
      "28         3.0  8062.775484  \n",
      "31         3.0  8156.156159  \n",
      "70         3.0  8177.640431  \n",
      "63         3.0  7820.656336  \n",
      "58         3.0  7827.533139  \n",
      "74         3.0  7827.533139  \n"
     ]
    }
   ],
   "source": [
    "# 2nd step of the loop:\n",
    "selectHashtags(job).run()\n",
    "# the signification of the displayed columns are:\n",
    "# count (= total number of occurrences),\n",
    "# label_init (= initial label before propagation, -1 means no initial labels)\n",
    "# vertex_id  (= ID of the vertex in the hashtag graph)\n",
    "# label_sum1 (= number of neighbors with label 1)\n",
    "# signi_sum1 (= sum of the significance of edges with neighbors having label 1)\n",
    "# label_sum2 (= number of neighbors with label 2)\n",
    "# signi_sum2 (= sum of the significance of edges with neighbors having label 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can now update the hashtag list and return the 1st step.\n",
    "job['htgs_lists'] = [['money', 'stocks', 'stockmarket', 'cash', 'market', 'ameritrade', 'scottrade'],\n",
    "               ['401k', 'finance', 'etrade', 'tradeking', 'stock', 'amtd', 'alerts']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.  update HT group in database\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "updateHTGroups(job).run()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:graphtool_compile]",
   "language": "python",
   "name": "conda-env-graphtool_compile-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
