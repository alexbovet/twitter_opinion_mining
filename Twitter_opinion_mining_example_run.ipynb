{
 "metadata" : {"author" : "Alexandre Bovet <alexandre.bovet@gmail.com>"},
               "license" : "BSD 3 clause"
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Opinion Mining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules and define filenames and directories for current job\n",
    "All the parameters are saved in a dictionary named `job`\n",
    "that will be passed to the different modules\n",
    "and that can be saved to reproduce the results.\n",
    "Optional parameters can be added to fine tune the\n",
    "process. Each optional parameter is explained at the corresponding step below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the different modules\n",
    "from buildDatabase import buildDatabse\n",
    "from makeHTnetwork import makeHTNetwork\n",
    "from selectInitialHashtags import selectInitialHashtags\n",
    "from propagateLabels import propagateLabels\n",
    "from addStatSigniHT import addStatSigniHT\n",
    "from selectHashtags import selectHashtags\n",
    "from updateHTGroups import updateHTGroups\n",
    "from buildTrainingSet import buildTrainingSet\n",
    "from crossValOptimize import crossValOptimize\n",
    "from trainClassifier import trainClassifier\n",
    "from classifyTweets import classifyTweets\n",
    "from makeProbaDF import makeProbaDF\n",
    "from analyzeProbaDF import analyzeProbaDF\n",
    "\n",
    "# create empty dictionary and add parameters\n",
    "job = {}\n",
    "# list of directories containing the tweet archive files (TAJ)\n",
    "job['tweet_archive_dirs'] = ['etrade']\n",
    "\n",
    "# SQLite database that will be created\n",
    "job['sqlite_db_filename'] = 'test.sqlite'\n",
    "\n",
    "# hashtag co-occurrence graph that will be created\n",
    "job['graph_file'] = 'graph_file.graphml'\n",
    "\n",
    "# pickle files where the training set features will be saved\n",
    "job['features_pickle_file'] = 'features.pickle'\n",
    "\n",
    "# pickle file where the training set labels will be saved\n",
    "job['labels_pickle_file'] = 'labels.pickle'\n",
    "\n",
    "# vectorized features file\n",
    "job['features_vect_file'] = 'features.mmap'\n",
    "\n",
    "# vectorized labels file\n",
    "job['labels_vect_file'] = 'labels.mmap'\n",
    "\n",
    "# mapping between labels names and numbers\n",
    "job['labels_mappers_file'] = 'labels_mappers.pickle'\n",
    "\n",
    "# JSON file with the classifier best parameters obtained from cross-validation\n",
    "job['best_params_file'] = 'best_params.json'\n",
    "\n",
    "# where the trained calssifier will be saved\n",
    "job['classifier_filename'] = 'classifier.pickle'\n",
    "\n",
    "# DataFrame with the results of the label propagation\n",
    "# on the hashtag network\n",
    "job['propag_results_filename'] = 'propag_results.pickle'\n",
    "\n",
    "# DataFrame with the classification probability of\n",
    "# every tweets in the database\n",
    "job['df_proba_filename'] = 'df_proba.pickle'\n",
    "\n",
    "# DataFrame with the number of tweets in each camp per day\n",
    "job['df_num_tweets_filename'] = 'df_num_tweets.pickle'\n",
    "\n",
    "# DataFrame with the number of users in each camp per day\n",
    "job['df_num_users_filename'] = 'df_num_users.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build the SQLite database with the extracted information from the tweets\n",
    "Read the tweets from all the .taj files in the directories `tweet_archive_dirs`\n",
    "and add them to the database `sqlite_db_filename`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping index sqlite_autoindex_hashtag_1\n",
      "index associated with UNIQUE or PRIMARY KEY constraint cannot be dropped\n",
      "Dropping index sqlite_autoindex_hashtag_tweet_user_1\n",
      "index associated with UNIQUE or PRIMARY KEY constraint cannot be dropped\n",
      "Dropping index sqlite_autoindex_tweet_to_keyword_1\n",
      "index associated with UNIQUE or PRIMARY KEY constraint cannot be dropped\n",
      "Dropping index sqlite_autoindex_tweet_to_mentioned_uid_1\n",
      "index associated with UNIQUE or PRIMARY KEY constraint cannot be dropped\n",
      "Dropping index sqlite_autoindex_filename_1\n",
      "index associated with UNIQUE or PRIMARY KEY constraint cannot be dropped\n",
      "Dropping index sqlite_autoindex_query_1\n",
      "index associated with UNIQUE or PRIMARY KEY constraint cannot be dropped\n",
      "Dropping index sqlite_autoindex_source_content_1\n",
      "index associated with UNIQUE or PRIMARY KEY constraint cannot be dropped\n",
      "Dropping index sqlite_autoindex_source_url_1\n",
      "index associated with UNIQUE or PRIMARY KEY constraint cannot be dropped\n",
      "Dropping index sqlite_autoindex_tweet_to_query_id_1\n",
      "index associated with UNIQUE or PRIMARY KEY constraint cannot be dropped\n",
      "Dropping index timestamp_index\n",
      "Dropping index user_index\n",
      "Dropping index retweet_timestamp_index\n",
      "Dropping index retweet_user_index\n",
      "Dropping index ht_count_index\n",
      "Dropping index tweet_id_index\n",
      "Dropping index hashtag_index\n",
      "Dropping index user_id_index\n",
      "Dropping index tweet_id_mention_index\n",
      "Dropping index keyword_index\n",
      "Dropping index retweet_author_index\n",
      "Dropping index mention_author_index\n",
      "Dropping index reply_author_index\n",
      "Dropping index quote_author_index\n",
      "Dropping index tweet_id_retweet_index\n",
      "Dropping index retweet_id_retweet_index\n",
      "Dropping index tweet_id_reply_index\n",
      "Dropping index tweet_id_quote_index\n",
      "Dropping index source_content_tweet_index\n",
      "Dropping index tweet_id_source_content\n",
      "Dropping index tweet_id_query_id_index\n",
      "Dropping index ht_class_supp_index\n",
      "0 over 1\n",
      "... getting data from etrade/tweets-b15b7e5b-a99f-4612-a24d-c452dbc0b9fb.taj\n",
      "... took 2.666s\n",
      "\n",
      "*** updating sqlite tables...\n",
      "*** took 0.3597s\n",
      "\n",
      "Finished\n",
      "Total time 3.02546s\n",
      "Transaction time 3.03604s\n",
      "Total time 3.03806s\n",
      "sqlite_file : test.sqlite\n",
      "Creating indexes\n",
      "time 0.219461s\n",
      "\n",
      "Number of tweets: [(16071,)]\n"
     ]
    }
   ],
   "source": [
    "buildDatabse(job).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Make the Hashtag co-occurrences network\n",
    "Reads all the co-occurences from the SQLite database and builds the network\n",
    "of where nodes are hashtags and edges are co-occurrences.\n",
    "The graph is a [*graph-tool*](https://graph-tool.skewed.de/) object and is saved in graphml format to `graph_file`.\n",
    "\n",
    "Nodes of the graph have two properties: `counts` is the number of single occurrences of the hashtag and `name` is the name of the hashtag.\n",
    "\n",
    "Edges have a property `weights` equal to the number of co-occurrences they represent.\n",
    "\n",
    "The graph has the following properties saved with it:\n",
    "- `Ntweets`: number of tweets with at least one hashtag used to build the graph.\n",
    "- `start_date` : date of the first tweet.\n",
    "- `stop_date` : date of the last tweet.\n",
    "- `weight_threshold` : co-occurrence threshold. Edges with less than `weight_threshold` co-occurrences are discarded.\n",
    "\n",
    "*Optional parameters that can be added to `job`:*\n",
    "- `start_date` and `stop_date` to specify a time range for the tweets. (Default is `None`, i.e. select all the tweets in the database).\n",
    "- `weight_threshold` is the minimum number of co-occurences between to hashtag to be included in the graph. (Default is 3).\n",
    "\n",
    "To add a parameter to job, simply execute `job[\"parameter name\"] = parameter value`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating edge list\n",
      "*** took 2.083s\n",
      "creating graph\n",
      "*** took 0.004655s\n",
      "\n",
      "Number of nodes: 158\n",
      "Number of edges: 351\n"
     ]
    }
   ],
   "source": [
    "makeHTNetwork(job).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Add statistical significance value to edges\n",
    "Adds a property `s` to edges of the graph corresponding to the statistical significance (`s = log10(p_0/p)`)\n",
    "of the co-occurence computed from a null model [1].\n",
    "The computation is done using `p0=1e-6` and `p0` is saved as a graph property.\n",
    "Different values of `p0` can be tested latter.\n",
    "The resulting graph is saved to `graph_file`.\n",
    "\n",
    "*Optional parameters that can be added to `job`:*\n",
    "- `ncpu` : number of processors to be used. (Default is the number of cores on your machine minus 1).\n",
    "\n",
    "\n",
    "[1] Martinez-Romo, J. et al. Disentangling categorical relationships through a graph of co-occurrences. Phys. Rev. E 84, 1â€“8 (2011).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing significance of links\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=7)]: Done 186 tasks      | elapsed:    1.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "*** took 38.93s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Done 351 out of 351 | elapsed:   38.7s finished\n"
     ]
    }
   ],
   "source": [
    "addStatSigniHT(job).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Select the initial hashtags to start the propagation\n",
    "This will display to top occurring hashtags.\n",
    "\n",
    "*Optional parameters that can be added to `job`:*\n",
    "- `num_top_htgs` : (Default is top 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Top 100 occuring hashtags:\n",
      "* rank: (name: frequency)\n",
      "0 :('finance', 8162)\n",
      "1 :('etrade', 8028)\n",
      "2 :('tradeking', 7663)\n",
      "3 :('money', 7436)\n",
      "4 :('stock', 4169)\n",
      "5 :('401k', 4141)\n",
      "6 :('amtd', 4141)\n",
      "7 :('alerts', 4141)\n",
      "8 :('stocks', 3937)\n",
      "9 :('stockmarket', 3915)\n",
      "10 :('cash', 3911)\n",
      "11 :('market', 3523)\n",
      "12 :('ameritrade', 3522)\n",
      "13 :('scottrade', 3522)\n",
      "14 :('mortgage', 714)\n",
      "15 :('rates', 352)\n",
      "16 :('didyouknow', 210)\n",
      "17 :('loan', 94)\n",
      "18 :('loans', 76)\n",
      "19 :('seattle', 70)\n",
      "20 :('history', 70)\n",
      "21 :('interest', 70)\n",
      "22 :('canadian', 68)\n",
      "23 :('motors', 58)\n",
      "24 :('kia', 58)\n",
      "25 :('hiring', 49)\n",
      "26 :('year', 47)\n",
      "27 :('house', 45)\n",
      "28 :('news', 45)\n",
      "29 :('a', 44)\n",
      "30 :('get', 44)\n",
      "31 :('major', 40)\n",
      "32 :('rate', 39)\n",
      "33 :('refinancing', 38)\n",
      "34 :('boycottcnn', 36)\n",
      "35 :('calculator', 35)\n",
      "36 :('lenders', 35)\n",
      "37 :('daytrading', 32)\n",
      "38 :('stocktrader', 31)\n",
      "39 :('trump', 30)\n",
      "40 :('vermont', 30)\n",
      "41 :('2nd', 29)\n",
      "42 :('mortgages', 29)\n",
      "43 :('estimate', 29)\n",
      "44 :('calcu', 27)\n",
      "45 :('ecommerce', 26)\n",
      "46 :('jobsearch', 26)\n",
      "47 :('alaska', 26)\n",
      "48 :('etrade4all', 22)\n",
      "49 :('direct', 22)\n",
      "50 :('nasdaq', 19)\n",
      "51 :('carrington', 19)\n",
      "52 :('services', 19)\n",
      "53 :('blackoutcnn', 19)\n",
      "54 :('amortization', 18)\n",
      "55 :('with', 18)\n",
      "56 :('invest', 18)\n",
      "57 :('homedepot', 18)\n",
      "58 :('indianapolis', 18)\n",
      "59 :('table', 18)\n",
      "60 :('school', 17)\n",
      "61 :('motorola', 17)\n",
      "62 :('usa', 16)\n",
      "63 :('entertainment', 16)\n",
      "64 :('prediction', 16)\n",
      "65 :('investing', 16)\n",
      "66 :('postseason', 16)\n",
      "67 :('air', 16)\n",
      "68 :('jobs', 16)\n",
      "69 :('tax', 16)\n",
      "70 :('jerseycitynj', 15)\n",
      "71 :('job', 15)\n",
      "72 :('jerseycity', 15)\n",
      "73 :('indiana', 14)\n",
      "74 :('fortwayne', 13)\n",
      "75 :('robinhood', 13)\n",
      "76 :('colleges', 13)\n",
      "77 :('subprime', 13)\n",
      "78 :('home', 13)\n",
      "79 :('www', 13)\n",
      "80 :('wallstreet', 12)\n",
      "81 :('bostonma', 12)\n",
      "82 :('jobssearch', 12)\n",
      "83 :('boston', 12)\n",
      "84 :('alterconf', 12)\n",
      "85 :('app', 12)\n",
      "86 :('boycottpepsi', 12)\n",
      "87 :('freedom', 12)\n",
      "88 :('print', 12)\n",
      "89 :('stloius', 12)\n",
      "90 :('predictions', 9)\n",
      "91 :('wellsfargo', 9)\n",
      "92 :('out', 9)\n",
      "93 :('gainers', 9)\n",
      "94 :('losers', 9)\n",
      "95 :('charts', 9)\n",
      "96 :('ilcoin', 8)\n",
      "97 :('geico', 8)\n",
      "98 :('sprint', 8)\n",
      "99 :('seo', 8)\n"
     ]
    }
   ],
   "source": [
    "selectInitialHashtags(job).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select seeds hashtags you want to use from the list (minimum two) \n",
    "and add them to the `job` dictionary with the key `initial_htgs_lists`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initial_htgs_lists is a list of list with hashtags seeds for each camp:\n",
    "job['initial_htgs_lists'] = [['mortgage'],\n",
    "                             ['etrade']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Propagate labels to neighboring hashtags\n",
    "This part can be looped by updating the `htgs_lists` in `job` with the result of the label propagation to reach a larger number of hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start with the hashtag seeds selected above.\n",
    "job['htgs_lists'] = job['initial_htgs_lists']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loop has two steps:\n",
    "1. `propagateLabels` uses the graph from `graph_file` and the initial hashtags from `htgs_lists` to propagate their labels to their neighbors taking into account the statistical significance of edges. The results are saved in a pandas DataFrame in `propag_results_filename`.\n",
    "    - *Optional parameters that can be added to `job`:*\n",
    "        - `count_ratio` : threshold, $r$, for removing hashtags with a number of single occurrences smaller than $r \\max\\limits_{v_j\\in C_k} c_j$ where $c_i$ is the number of occurrences of the hashtag associated with vertex $v_i$, $C_k$ is the class to which $v_i$ belong. (Default = 0.001).\n",
    "        - `p0` : significance threshold. to keep only edges with p_val <= p0. (Default = 1e-5).\n",
    "\n",
    "2. Visualisation of the results using `selectHashtags`, and updating the `htgs_lists` list. This will print a list of hashtags, $i$, for each camp $C_k$ satisfying: $\\sum_{j \\in C_k} s_{ij} > \\sum_{j \\notin C_k} s_{ij}$, where $\\{i : i \\notin C_k \\}$ represents all the other camps than $C_k$.\n",
    "    - *Optional parameters that can be added to `job`:*\n",
    "        - `num_top_htgs` : number of top hashtags to be displayed in each camp. (Default is 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propagating labels\n",
      "saving results\n"
     ]
    }
   ],
   "source": [
    "# 1st step of the loop:\n",
    "propagateLabels(job).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " +++ hashtags in camp 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>count</th>\n",
       "      <th>label_init</th>\n",
       "      <th>vertex_id</th>\n",
       "      <th>label_sum2</th>\n",
       "      <th>signi_sum2</th>\n",
       "      <th>label_sum1</th>\n",
       "      <th>signi_sum1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>tradeking</td>\n",
       "      <td>7663</td>\n",
       "      <td>-1</td>\n",
       "      <td>92</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3349.239254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>stock</td>\n",
       "      <td>4169</td>\n",
       "      <td>-1</td>\n",
       "      <td>71</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1161.203487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>401k</td>\n",
       "      <td>4141</td>\n",
       "      <td>-1</td>\n",
       "      <td>84</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1189.715525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>amtd</td>\n",
       "      <td>4141</td>\n",
       "      <td>-1</td>\n",
       "      <td>97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1189.715525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>alerts</td>\n",
       "      <td>4141</td>\n",
       "      <td>-1</td>\n",
       "      <td>111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1189.715525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>market</td>\n",
       "      <td>3523</td>\n",
       "      <td>-1</td>\n",
       "      <td>91</td>\n",
       "      <td>1.0</td>\n",
       "      <td>966.317201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>ameritrade</td>\n",
       "      <td>3522</td>\n",
       "      <td>-1</td>\n",
       "      <td>82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>965.973213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>scottrade</td>\n",
       "      <td>3522</td>\n",
       "      <td>-1</td>\n",
       "      <td>107</td>\n",
       "      <td>1.0</td>\n",
       "      <td>965.973213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>daytrading</td>\n",
       "      <td>32</td>\n",
       "      <td>-1</td>\n",
       "      <td>103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.846667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>stocktrader</td>\n",
       "      <td>31</td>\n",
       "      <td>-1</td>\n",
       "      <td>104</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.978751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name  count  label_init  vertex_id  label_sum2   signi_sum2  label_sum1  signi_sum1\n",
       "92     tradeking   7663          -1         92         1.0  3349.239254         0.0         0.0\n",
       "71         stock   4169          -1         71         1.0  1161.203487         0.0         0.0\n",
       "84          401k   4141          -1         84         1.0  1189.715525         0.0         0.0\n",
       "97          amtd   4141          -1         97         1.0  1189.715525         0.0         0.0\n",
       "111       alerts   4141          -1        111         1.0  1189.715525         0.0         0.0\n",
       "91        market   3523          -1         91         1.0   966.317201         0.0         0.0\n",
       "82    ameritrade   3522          -1         82         1.0   965.973213         0.0         0.0\n",
       "107    scottrade   3522          -1        107         1.0   965.973213         0.0         0.0\n",
       "103   daytrading     32          -1        103         1.0     0.846667         0.0         0.0\n",
       "104  stocktrader     31          -1        104         1.0     1.978751         0.0         0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " +++ hashtags in camp 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>count</th>\n",
       "      <th>label_init</th>\n",
       "      <th>vertex_id</th>\n",
       "      <th>label_sum2</th>\n",
       "      <th>signi_sum2</th>\n",
       "      <th>label_sum1</th>\n",
       "      <th>signi_sum1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rates</td>\n",
       "      <td>352</td>\n",
       "      <td>-1</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>307.584428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>loan</td>\n",
       "      <td>94</td>\n",
       "      <td>-1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>117.557700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>loans</td>\n",
       "      <td>76</td>\n",
       "      <td>-1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.523145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>seattle</td>\n",
       "      <td>70</td>\n",
       "      <td>-1</td>\n",
       "      <td>33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85.740185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>canadian</td>\n",
       "      <td>68</td>\n",
       "      <td>-1</td>\n",
       "      <td>66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83.105606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>year</td>\n",
       "      <td>47</td>\n",
       "      <td>-1</td>\n",
       "      <td>145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.595368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>news</td>\n",
       "      <td>45</td>\n",
       "      <td>-1</td>\n",
       "      <td>35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.056316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>a</td>\n",
       "      <td>44</td>\n",
       "      <td>-1</td>\n",
       "      <td>121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51.687805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>get</td>\n",
       "      <td>44</td>\n",
       "      <td>-1</td>\n",
       "      <td>122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51.687805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>rate</td>\n",
       "      <td>39</td>\n",
       "      <td>-1</td>\n",
       "      <td>43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.187496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>refinancing</td>\n",
       "      <td>38</td>\n",
       "      <td>-1</td>\n",
       "      <td>44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43.889269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>calculator</td>\n",
       "      <td>35</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.187404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>lenders</td>\n",
       "      <td>35</td>\n",
       "      <td>-1</td>\n",
       "      <td>54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.998244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>vermont</td>\n",
       "      <td>30</td>\n",
       "      <td>-1</td>\n",
       "      <td>120</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.753301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33.525327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>estimate</td>\n",
       "      <td>29</td>\n",
       "      <td>-1</td>\n",
       "      <td>146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.232553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>calcu</td>\n",
       "      <td>27</td>\n",
       "      <td>-1</td>\n",
       "      <td>127</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.648810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>alaska</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>113</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.851720</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.357837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>direct</td>\n",
       "      <td>22</td>\n",
       "      <td>-1</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.199923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>services</td>\n",
       "      <td>19</td>\n",
       "      <td>-1</td>\n",
       "      <td>131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.337737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>carrington</td>\n",
       "      <td>19</td>\n",
       "      <td>-1</td>\n",
       "      <td>130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.337737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>indianapolis</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "      <td>63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.051527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>table</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "      <td>86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.051527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>homedepot</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.051527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>with</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.051527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>amortization</td>\n",
       "      <td>18</td>\n",
       "      <td>-1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.051527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>indiana</td>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "      <td>58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.912591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>colleges</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.629329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>subprime</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.629329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>fortwayne</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.629329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>home</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.629329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>ask</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.221801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>canada</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.942047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  count  label_init  vertex_id  label_sum2  signi_sum2  label_sum1  signi_sum1\n",
       "25          rates    352          -1         25         0.0    0.000000         1.0  307.584428\n",
       "26           loan     94          -1         26         0.0    0.000000         1.0  117.557700\n",
       "18          loans     76          -1         18         0.0    0.000000         1.0   14.523145\n",
       "33        seattle     70          -1         33         0.0    0.000000         1.0   85.740185\n",
       "66       canadian     68          -1         66         0.0    0.000000         1.0   83.105606\n",
       "145          year     47          -1        145         0.0    0.000000         1.0   55.595368\n",
       "35           news     45          -1         35         0.0    0.000000         1.0   50.056316\n",
       "121             a     44          -1        121         0.0    0.000000         1.0   51.687805\n",
       "122           get     44          -1        122         0.0    0.000000         1.0   51.687805\n",
       "43           rate     39          -1         43         0.0    0.000000         1.0   45.187496\n",
       "44    refinancing     38          -1         44         0.0    0.000000         1.0   43.889269\n",
       "5      calculator     35          -1          5         0.0    0.000000         1.0   30.187404\n",
       "54        lenders     35          -1         54         0.0    0.000000         1.0   39.998244\n",
       "120       vermont     30          -1        120         1.0    1.753301         1.0   33.525327\n",
       "146      estimate     29          -1        146         0.0    0.000000         1.0   32.232553\n",
       "127         calcu     27          -1        127         0.0    0.000000         1.0   29.648810\n",
       "113        alaska     26          -1        113         1.0    0.851720         1.0   28.357837\n",
       "21         direct     22          -1         21         0.0    0.000000         1.0   23.199923\n",
       "131      services     19          -1        131         0.0    0.000000         1.0   19.337737\n",
       "130    carrington     19          -1        130         0.0    0.000000         1.0   19.337737\n",
       "63   indianapolis     18          -1         63         0.0    0.000000         1.0   18.051527\n",
       "86          table     18          -1         86         0.0    0.000000         1.0   18.051527\n",
       "55      homedepot     18          -1         55         0.0    0.000000         1.0   18.051527\n",
       "29           with     18          -1         29         0.0    0.000000         1.0   18.051527\n",
       "28   amortization     18          -1         28         0.0    0.000000         1.0   18.051527\n",
       "58        indiana     14          -1         58         0.0    0.000000         1.0   12.912591\n",
       "36       colleges     13          -1         36         0.0    0.000000         1.0   11.629329\n",
       "137      subprime     13          -1        137         0.0    0.000000         1.0   11.629329\n",
       "24      fortwayne     13          -1         24         0.0    0.000000         1.0   11.629329\n",
       "152          home     13          -1        152         0.0    0.000000         1.0   11.629329\n",
       "89            ask      8          -1         89         0.0    0.000000         1.0    5.221801\n",
       "154        canada      7          -1        154         0.0    0.000000         1.0    3.942047"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2nd step of the loop:\n",
    "selectHashtags(job).run()\n",
    "# the signification of the displayed columns are:\n",
    "# count (= total number of occurrences),\n",
    "# label_init (= initial label before propagation, -1 means no initial labels)\n",
    "# vertex_id  (= ID of the vertex in the hashtag graph)\n",
    "# label_sum1 (= number of neighbors with label 1)\n",
    "# signi_sum1 (= sum of the significance of edges with neighbors having label 1)\n",
    "# label_sum2 (= number of neighbors with label 2)\n",
    "# signi_sum2 (= sum of the significance of edges with neighbors having label 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can now update the hashtag list and return the 1st step.\n",
    "job['htgs_lists'] = [['mortgage', 'rates', 'loan', 'loans', 'lenders', 'amortization', 'subprime'],\n",
    "               ['etrade', 'tradeking', 'stock', '401k', 'market', 'ameritrade', 'scottrade']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "job['htgs_lists'] = [['mortgage', 'rates', 'loan', 'loans', 'lenders', 'amortization', 'subprime','interest'],\n",
    "               ['etrade', 'tradeking', 'stock', '401k', 'market', 'ameritrade', 'scottrade', 'finance', 'money']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.  Mark the selected hashtags in the database and build the training set\n",
    "`updateHTGroups` takes the lists of hashtags `htgs_lists` and mark then in the database `sqlite_db_filename`.\n",
    "\n",
    "*Optional parameters that can be added to `job`:*\n",
    "- `column_name_ht_group` : name of the column added to the database (Default is `'ht_class'`). Different names can be used to test different `htgs_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column ht_class already exists, column values will be replaced.\n",
      "*** took 0.1333s\n"
     ]
    }
   ],
   "source": [
    "updateHTGroups(job).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`buildTrainingSet` reads tweets from the database with hashtags marked above, extract the features and labels of each tweets and saves them in `features_pickle_file` and `labels_pickle_file`, respectively.\n",
    "Vectorized versions of the features and labels are saved to `features_vect_file` and `labels_vect_file` for the cross-validation. A mapper between label names and label number is saved to `labels_mappers_file`.\n",
    "\n",
    "*Optional parameters:*\n",
    "\n",
    "- If the optional parameter `column_name_ht_group` has been changed in `job` in the step before, it will be used here to select the corresponding hashtag lists.\n",
    "- `undersample_maj_class` : whether to undersample the majority class in order to balance the training set. Default is True, if False, unbalanced training set will be used and [class weight](http://scikit-learn.org/0.18/modules/generated/sklearn.linear_model.SGDClassifier.html) will be adjusted accrodingly during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tweets 1: 40\n",
      "Num tweets 2: 81\n",
      "Balancing sets by undersampling the majority class\n",
      "\n",
      "Vectorizing features\n",
      "*** took 0.002084s\n",
      "Num samples x Num features\n",
      "(80, 1113)\n"
     ]
    }
   ],
   "source": [
    "buildTrainingSet(job).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Cross-Validation\n",
    "Estimate the performance of the classifier and optimize classifier parameters with cross-validation. `crossValOptimize` loads the vectorized features and labels (`features_vect_file` and `labels_vect_file`) and saves the results of the optimization to `best_params_file` in JSON format.\n",
    "\n",
    "*Optional parameters:*\n",
    "- if `undersample_maj_class` was set to `False` when building the training set, class weights will be adjusted to take into account different sizes of classes.\n",
    "- `ncpu` : number of cores to use (default is the number of cpus on your machine minus one).\n",
    "- `scoring` : The score used to optimize (default is `'f1_micro'`). See the [documentation](http://scikit-learn.org/0.18/modules/generated/sklearn.model_selection.GridSearchCV.html) for explanation and other possibilities. \n",
    "- `n_splits` : number of [folds](http://scikit-learn.org/0.18/modules/generated/sklearn.model_selection.KFold.html) (default is 10).\n",
    "- `loss` : [loss function](scikit-learn.org/0.18/modules/generated/sklearn.linear_model.SGDClassifier.html) to be used. Default is `'log'` for Logistic Regression.\n",
    "- `penalty` : [penalty](scikit-learn.org/0.18/modules/generated/sklearn.linear_model.SGDClassifier.html) of the regularization term (default is `'l2`).\n",
    "- `n_iter` : [number of iterations](scikit-learn.org/0.18/modules/generated/sklearn.linear_model.SGDClassifier.html) of the gradient descent algorithm. Default is `5e5/(number of training samples)`. See the sklearn Stochastic Gradient Descent [user guide](http://scikit-learn.org/0.18/modules/sgd.html#sgd) for recommended settings.\n",
    "- `grid_search_parameters` : parameter space to explore during the cross-validation. Default is `{'classifier__alpha' : np.logspace(-1,-7, num=20)}`, i.e. optimizing the [regularization strength](http://scikit-learn.org/0.18/modules/sgd.html#sgd) (`alpha`) between 1e-1 and 1e-7 with 20 logarithmic steps.\n",
    "- `verbose` : verbosity level of the calssifier (default is 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing grid search...\n",
      "pipeline: ['classifier']\n",
      "parameters:\n",
      "{'classifier__alpha': array([  1.00000000e-01,   4.83293024e-02,   2.33572147e-02,\n",
      "         1.12883789e-02,   5.45559478e-03,   2.63665090e-03,\n",
      "         1.27427499e-03,   6.15848211e-04,   2.97635144e-04,\n",
      "         1.43844989e-04,   6.95192796e-05,   3.35981829e-05,\n",
      "         1.62377674e-05,   7.84759970e-06,   3.79269019e-06,\n",
      "         1.83298071e-06,   8.85866790e-07,   4.28133240e-07,\n",
      "         2.06913808e-07,   1.00000000e-07])}\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 2.11, NNZs: 1052, Bias: 0.010545, T: 72, Avg. loss: 0.282978\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 1.77, NNZs: 1052, Bias: 0.012035, T: 144, Avg. loss: 0.224334\n",
      "Norm: 2.18, NNZs: 1021, Bias: 0.015712, T: 72, Avg. loss: 0.316783\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 2.19, NNZs: 1064, Bias: 0.015586, T: 72, Avg. loss: 0.322518\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.81, NNZs: 1064, Bias: 0.017748, T: 144, Avg. loss: 0.247319\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 2.16, NNZs: 995, Bias: 0.024135, T: 72, Avg. loss: 0.308096\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Norm: 2.10, NNZs: 954, Bias: 0.020799, T: 72, Avg. loss: 0.285011\n",
      "Norm: 2.17, NNZs: 1054, Bias: 0.013819, T: 72, Avg. loss: 0.303323\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 1.79, NNZs: 1021, Bias: 0.017550, T: 144, Avg. loss: 0.242373\n",
      "Norm: 2.15, NNZs: 972, Bias: 0.012214, T: 72, Avg. loss: 0.318290\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 2.13, NNZs: 1002, Bias: 0.010893, T: 72, Avg. loss: 0.303160\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 1.76, NNZs: 954, Bias: 0.021932, T: 144, Avg. loss: 0.223202\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 1.79, NNZs: 972, Bias: 0.014101, T: 144, Avg. loss: 0.242178\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 2.15, NNZs: 1086, Bias: 0.014341, T: 72, Avg. loss: 0.302237\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 1.77, NNZs: 1002, Bias: 0.012128, T: 144, Avg. loss: 0.233469\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Norm: 1.80, NNZs: 995, Bias: 0.025819, T: 144, Avg. loss: 0.239007\n",
      "Norm: 3.36, NNZs: 1052, Bias: 0.034565, T: 72, Avg. loss: 0.241062\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.51, NNZs: 1052, Bias: 0.035294, T: 144, Avg. loss: 0.167664\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 3.49, NNZs: 1002, Bias: 0.041439, T: 72, Avg. loss: 0.256299\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.52, NNZs: 1002, Bias: 0.042421, T: 144, Avg. loss: 0.171605\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 1.79, NNZs: 1086, Bias: 0.015766, T: 144, Avg. loss: 0.232850\n",
      "Norm: 1.79, NNZs: 1054, Bias: 0.015442, T: 144, Avg. loss: 0.231452\n",
      "Total training time: 0.02 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 3.51, NNZs: 1064, Bias: 0.031611, T: 72, Avg. loss: 0.270049\n",
      "-- Epoch 1\n",
      "Norm: 2.15, NNZs: 1037, Bias: 0.016260, T: 72, Avg. loss: 0.308302\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 3.51, NNZs: 1054, Bias: 0.030389, T: 72, Avg. loss: 0.260544\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 5.78, NNZs: 1086, Bias: 0.037358, T: 72, Avg. loss: 0.245977\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "Norm: 2.52, NNZs: 1054, Bias: 0.031307, T: 144, Avg. loss: 0.172519\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 1.78, NNZs: 1037, Bias: 0.018873, T: 144, Avg. loss: 0.238510\n",
      "Norm: 11.05, NNZs: 995, Bias: 0.070518, T: 72, Avg. loss: 0.333352\n",
      "Norm: 3.40, NNZs: 972, Bias: 0.022906, T: 72, Avg. loss: 0.258064\n",
      "Total training time: 0.03 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "Norm: 3.71, NNZs: 1086, Bias: 0.038058, T: 144, Avg. loss: 0.143965\n",
      "Norm: 2.56, NNZs: 1064, Bias: 0.033910, T: 144, Avg. loss: 0.183136\n",
      "-- Epoch 2\n",
      "Norm: 2.50, NNZs: 972, Bias: 0.025323, T: 144, Avg. loss: 0.176380\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Total training time: 0.02 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 5.76, NNZs: 954, Bias: 0.027881, T: 72, Avg. loss: 0.224356\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 27.74, NNZs: 1002, Bias: 0.086539, T: 72, Avg. loss: 0.422549\n",
      "Norm: 3.69, NNZs: 954, Bias: 0.027777, T: 144, Avg. loss: 0.134539\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 3.44, NNZs: 1021, Bias: 0.024018, T: 72, Avg. loss: 0.263392\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 19.12, NNZs: 1002, Bias: 0.087708, T: 144, Avg. loss: 0.213349\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 6.75, NNZs: 995, Bias: 0.071522, T: 144, Avg. loss: 0.178498\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 2.51, NNZs: 1021, Bias: 0.025117, T: 144, Avg. loss: 0.178913\n",
      "Norm: 32.23, NNZs: 1052, Bias: 0.083967, T: 72, Avg. loss: 0.623674\n",
      "Norm: 10.06, NNZs: 1037, Bias: 0.061915, T: 72, Avg. loss: 0.276400\n",
      "-- Epoch 1\n",
      "Norm: 6.33, NNZs: 995, Bias: 0.070655, T: 72, Avg. loss: 0.377846\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 45.04, NNZs: 1064, Bias: 0.106727, T: 72, Avg. loss: 0.792162\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 65.67, NNZs: 972, Bias: 0.145203, T: 72, Avg. loss: 0.951502\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 22.14, NNZs: 1052, Bias: 0.086467, T: 144, Avg. loss: 0.312806\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "Norm: 54.13, NNZs: 972, Bias: 0.155927, T: 144, Avg. loss: 0.477746\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 3.29, NNZs: 1086, Bias: 0.044083, T: 72, Avg. loss: 0.228954\n",
      "-- Epoch 1\n",
      "Total training time: 0.02 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 34.00, NNZs: 1064, Bias: 0.112100, T: 144, Avg. loss: 0.397941\n",
      "Norm: 6.16, NNZs: 1037, Bias: 0.064471, T: 144, Avg. loss: 0.147568\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.49, NNZs: 1086, Bias: 0.044844, T: 144, Avg. loss: 0.161383\n",
      "Norm: 3.97, NNZs: 995, Bias: 0.071482, T: 144, Avg. loss: 0.210835\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 28.31, NNZs: 1064, Bias: 0.097826, T: 72, Avg. loss: 0.535548\n",
      "-- Epoch 1\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 5.78, NNZs: 1037, Bias: 0.040554, T: 72, Avg. loss: 0.311637\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "Norm: 3.71, NNZs: 1037, Bias: 0.042970, T: 144, Avg. loss: 0.178914\n",
      "Norm: 19.49, NNZs: 1064, Bias: 0.100439, T: 144, Avg. loss: 0.269412\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 62.79, NNZs: 1021, Bias: 0.126708, T: 72, Avg. loss: 0.761274\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 3.23, NNZs: 954, Bias: 0.022207, T: 72, Avg. loss: 0.223738\n",
      "Norm: 27.82, NNZs: 1054, Bias: 0.105325, T: 72, Avg. loss: 0.492695\n",
      "Norm: 37.84, NNZs: 1054, Bias: 0.123607, T: 72, Avg. loss: 0.745855\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 19.16, NNZs: 1054, Bias: 0.104725, T: 144, Avg. loss: 0.248062\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 28.57, NNZs: 1054, Bias: 0.120739, T: 144, Avg. loss: 0.374402\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 44.18, NNZs: 972, Bias: 0.120885, T: 72, Avg. loss: 0.579700\n",
      "-- Epoch 1\n",
      "Norm: 27.16, NNZs: 972, Bias: 0.104310, T: 72, Avg. loss: 0.373088\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 2.44, NNZs: 954, Bias: 0.023813, T: 144, Avg. loss: 0.158066\n",
      "Norm: 33.29, NNZs: 972, Bias: 0.122535, T: 144, Avg. loss: 0.290373\n",
      "Norm: 17.23, NNZs: 1002, Bias: 0.054265, T: 72, Avg. loss: 0.303757\n",
      "-- Epoch 2\n",
      "Total training time: 0.03 seconds.\n",
      "Total training time: 0.03 seconds.\n",
      "Norm: 21.71, NNZs: 972, Bias: 0.109594, T: 144, Avg. loss: 0.241580\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 3.35, NNZs: 995, Bias: 0.038816, T: 72, Avg. loss: 0.315391\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 51.46, NNZs: 1021, Bias: 0.127482, T: 144, Avg. loss: 0.380756\n",
      "-- Epoch 2\n",
      "Norm: 2.54, NNZs: 995, Bias: 0.041792, T: 144, Avg. loss: 0.213767\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 81.19, NNZs: 1086, Bias: 0.214362, T: 72, Avg. loss: 0.512080\n",
      "Norm: 10.83, NNZs: 1002, Bias: 0.085448, T: 72, Avg. loss: 0.300230\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 44.36, NNZs: 1021, Bias: 0.160784, T: 72, Avg. loss: 0.613013\n",
      "Total training time: 0.04 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 10.97, NNZs: 1002, Bias: 0.056072, T: 144, Avg. loss: 0.155885\n",
      "Norm: 30.10, NNZs: 1021, Bias: 0.099489, T: 72, Avg. loss: 0.633320\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Total training time: 0.03 seconds.\n",
      "Norm: 3.50, NNZs: 1037, Bias: 0.023815, T: 72, Avg. loss: 0.281921\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 73.40, NNZs: 1086, Bias: 0.153461, T: 144, Avg. loss: 0.515560\n",
      "Norm: 7.15, NNZs: 1002, Bias: 0.090916, T: 144, Avg. loss: 0.179350\n",
      "Norm: 20.74, NNZs: 1021, Bias: 0.101150, T: 144, Avg. loss: 0.319018\n",
      "-- Epoch 2\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 34.72, NNZs: 1021, Bias: 0.181407, T: 144, Avg. loss: 0.313413\n",
      "Norm: 60.27, NNZs: 1086, Bias: 0.173097, T: 72, Avg. loss: 0.426191\n",
      "Norm: 14.40, NNZs: 1052, Bias: 0.061316, T: 72, Avg. loss: 0.219332\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 28.93, NNZs: 1086, Bias: 0.081203, T: 72, Avg. loss: 0.446717\n",
      "Norm: 90.87, NNZs: 954, Bias: 0.242999, T: 72, Avg. loss: 1.336156\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 79.68, NNZs: 954, Bias: 0.243268, T: 144, Avg. loss: 0.668109\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 2.53, NNZs: 1037, Bias: 0.025806, T: 144, Avg. loss: 0.187169\n",
      "-- Epoch 2\n",
      "Norm: 19.94, NNZs: 1086, Bias: 0.083297, T: 144, Avg. loss: 0.225523\n",
      "Norm: 51.18, NNZs: 1086, Bias: 0.130249, T: 144, Avg. loss: 0.395456\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Norm: 41.89, NNZs: 1086, Bias: 0.105289, T: 72, Avg. loss: 0.592408\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 89.70, NNZs: 995, Bias: 0.165344, T: 72, Avg. loss: 1.220305\n",
      "Norm: 27.68, NNZs: 954, Bias: 0.099136, T: 72, Avg. loss: 0.497932\n",
      "-- Epoch 2\n",
      "Norm: 9.11, NNZs: 1052, Bias: 0.043082, T: 72, Avg. loss: 0.301936\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 55.33, NNZs: 995, Bias: 0.131874, T: 144, Avg. loss: 0.488858\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 31.60, NNZs: 1086, Bias: 0.107412, T: 144, Avg. loss: 0.297321\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.01, NNZs: 1052, Bias: 0.067040, T: 144, Avg. loss: 0.145709\n",
      "Norm: 5.70, NNZs: 1052, Bias: 0.045346, T: 144, Avg. loss: 0.162262\n",
      "Total training time: 0.02 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 66.28, NNZs: 954, Bias: 0.200908, T: 72, Avg. loss: 0.851609\n",
      "Norm: 19.07, NNZs: 954, Bias: 0.099432, T: 144, Avg. loss: 0.250612\n",
      "-- Epoch 2\n",
      "Norm: 5.84, NNZs: 1002, Bias: 0.037359, T: 72, Avg. loss: 0.232729\n",
      "Norm: 40.04, NNZs: 954, Bias: 0.083286, T: 72, Avg. loss: 0.591032\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 19.27, NNZs: 1064, Bias: 0.080650, T: 72, Avg. loss: 0.406479\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 78.69, NNZs: 995, Bias: 0.169587, T: 144, Avg. loss: 0.610659\n",
      "Norm: 54.38, NNZs: 954, Bias: 0.206027, T: 144, Avg. loss: 0.426630\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 3.73, NNZs: 1002, Bias: 0.038000, T: 144, Avg. loss: 0.138287\n",
      "Norm: 10.48, NNZs: 1064, Bias: 0.059136, T: 72, Avg. loss: 0.309770\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 12.20, NNZs: 1064, Bias: 0.083559, T: 144, Avg. loss: 0.206445\n",
      "Norm: 30.24, NNZs: 954, Bias: 0.088522, T: 144, Avg. loss: 0.296937\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 67.51, NNZs: 995, Bias: 0.131153, T: 72, Avg. loss: 0.977481\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 45.69, NNZs: 995, Bias: 0.101454, T: 72, Avg. loss: 0.663314\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 6.47, NNZs: 1064, Bias: 0.062824, T: 144, Avg. loss: 0.167608\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 16.50, NNZs: 1054, Bias: 0.080700, T: 72, Avg. loss: 0.206022\n",
      "Norm: 28.30, NNZs: 995, Bias: 0.121707, T: 72, Avg. loss: 0.663384\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 5.84, NNZs: 1052, Bias: 0.042256, T: 72, Avg. loss: 0.299213\n",
      "Norm: 34.58, NNZs: 995, Bias: 0.111549, T: 144, Avg. loss: 0.334037\n",
      "Norm: 11.14, NNZs: 1054, Bias: 0.069211, T: 72, Avg. loss: 0.406441\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 10.56, NNZs: 1054, Bias: 0.084327, T: 144, Avg. loss: 0.107923\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 70.94, NNZs: 1037, Bias: 0.174335, T: 72, Avg. loss: 0.738658\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 19.51, NNZs: 995, Bias: 0.124029, T: 144, Avg. loss: 0.333918\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 53.19, NNZs: 1037, Bias: 0.141264, T: 72, Avg. loss: 0.619330\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.75, NNZs: 1052, Bias: 0.043533, T: 144, Avg. loss: 0.173785\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 17.65, NNZs: 972, Bias: 0.073297, T: 72, Avg. loss: 0.316663\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 6.75, NNZs: 1054, Bias: 0.070373, T: 144, Avg. loss: 0.213351\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.27, NNZs: 972, Bias: 0.074070, T: 144, Avg. loss: 0.163618\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 41.18, NNZs: 1037, Bias: 0.134111, T: 72, Avg. loss: 0.873133\n",
      "Norm: 73.91, NNZs: 1037, Bias: 0.179012, T: 144, Avg. loss: 0.503971\n",
      "Norm: 51.52, NNZs: 1037, Bias: 0.148907, T: 144, Avg. loss: 0.410621\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 10.18, NNZs: 972, Bias: 0.053091, T: 72, Avg. loss: 0.325564\n",
      "-- Epoch 2\n",
      "Norm: 30.12, NNZs: 1037, Bias: 0.085838, T: 72, Avg. loss: 0.710303\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 15.66, NNZs: 1021, Bias: 0.080428, T: 72, Avg. loss: 0.190754\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Total training time: 0.02 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 10.07, NNZs: 1021, Bias: 0.082565, T: 144, Avg. loss: 0.101395\n",
      "-- Epoch 2\n",
      "Norm: 6.24, NNZs: 972, Bias: 0.055908, T: 144, Avg. loss: 0.173364\n",
      "-- Epoch 1\n",
      "Norm: 31.07, NNZs: 1037, Bias: 0.136465, T: 144, Avg. loss: 0.437612\n",
      "Norm: 20.72, NNZs: 1037, Bias: 0.090017, T: 144, Avg. loss: 0.357124\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 5.69, NNZs: 1064, Bias: 0.059036, T: 72, Avg. loss: 0.285157\n",
      "Norm: 74.11, NNZs: 1002, Bias: 0.123206, T: 72, Avg. loss: 0.558733\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "Norm: 64.99, NNZs: 1002, Bias: 0.123942, T: 144, Avg. loss: 0.279452\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 94.47, NNZs: 1002, Bias: 0.136945, T: 72, Avg. loss: 0.658010\n",
      "Norm: 16.99, NNZs: 1086, Bias: 0.077479, T: 72, Avg. loss: 0.421438\n",
      "Norm: 3.76, NNZs: 1064, Bias: 0.063091, T: 144, Avg. loss: 0.171529\n",
      "-- Epoch 1\n",
      "Norm: 46.09, NNZs: 1002, Bias: 0.121551, T: 72, Avg. loss: 0.726456\n",
      "-- Epoch 1\n",
      "Norm: 63.20, NNZs: 1002, Bias: 0.198163, T: 72, Avg. loss: 0.962760\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 10.00, NNZs: 1021, Bias: 0.055047, T: 72, Avg. loss: 0.423779\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "Norm: 51.99, NNZs: 1002, Bias: 0.206888, T: 144, Avg. loss: 0.482884\n",
      "-- Epoch 2\n",
      "Norm: 6.17, NNZs: 1021, Bias: 0.059809, T: 144, Avg. loss: 0.223108\n",
      "Norm: 85.67, NNZs: 1052, Bias: 0.253462, T: 72, Avg. loss: 1.138464\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 86.96, NNZs: 1002, Bias: 0.138365, T: 144, Avg. loss: 0.329128\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 10.90, NNZs: 1086, Bias: 0.077509, T: 144, Avg. loss: 0.216750\n",
      "Norm: 75.12, NNZs: 1052, Bias: 0.253559, T: 144, Avg. loss: 0.569243\n",
      "-- Epoch 2\n",
      "Norm: 6.19, NNZs: 1054, Bias: 0.054890, T: 72, Avg. loss: 0.245864\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 34.90, NNZs: 1002, Bias: 0.133072, T: 144, Avg. loss: 0.365833\n",
      "-- Epoch 1\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.86, NNZs: 1054, Bias: 0.056146, T: 144, Avg. loss: 0.142729\n",
      "Norm: 65.44, NNZs: 1052, Bias: 0.132912, T: 72, Avg. loss: 1.049400\n",
      "Norm: 16.86, NNZs: 954, Bias: 0.068327, T: 72, Avg. loss: 0.446206\n",
      "Norm: 110.32, NNZs: 1052, Bias: 0.314905, T: 72, Avg. loss: 1.404307\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 78.42, NNZs: 1064, Bias: 0.187212, T: 72, Avg. loss: 1.326791\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 10.79, NNZs: 954, Bias: 0.072797, T: 144, Avg. loss: 0.228449\n",
      "-- Epoch 2\n",
      "Norm: 101.53, NNZs: 1052, Bias: 0.315064, T: 144, Avg. loss: 0.702167\n",
      "Norm: 68.76, NNZs: 1064, Bias: 0.187489, T: 144, Avg. loss: 0.663429\n",
      "Norm: 10.76, NNZs: 1086, Bias: 0.063881, T: 72, Avg. loss: 0.310376\n",
      "Norm: 53.63, NNZs: 1052, Bias: 0.133628, T: 144, Avg. loss: 0.524847\n",
      "Norm: 36.41, NNZs: 1052, Bias: 0.091157, T: 72, Avg. loss: 0.524621\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 6.54, NNZs: 1086, Bias: 0.066911, T: 144, Avg. loss: 0.163864\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 100.50, NNZs: 1064, Bias: 0.222793, T: 72, Avg. loss: 1.627453\n",
      "Norm: 17.31, NNZs: 995, Bias: 0.077300, T: 72, Avg. loss: 0.485457\n",
      "-- Epoch 1\n",
      "Norm: 5.93, NNZs: 972, Bias: 0.044431, T: 72, Avg. loss: 0.298528\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 10.27, NNZs: 954, Bias: 0.050402, T: 72, Avg. loss: 0.400487\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Norm: 88.19, NNZs: 1054, Bias: 0.210304, T: 72, Avg. loss: 1.197374\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.30, NNZs: 954, Bias: 0.052013, T: 144, Avg. loss: 0.211548\n",
      "-- Epoch 2\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 77.32, NNZs: 1054, Bias: 0.210486, T: 144, Avg. loss: 0.598707\n",
      "Norm: 31.97, NNZs: 1052, Bias: 0.107267, T: 144, Avg. loss: 0.294486\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 92.50, NNZs: 1064, Bias: 0.223266, T: 144, Avg. loss: 0.813767\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.77, NNZs: 972, Bias: 0.046379, T: 144, Avg. loss: 0.171899\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 109.58, NNZs: 995, Bias: 0.287782, T: 72, Avg. loss: 1.041227\n",
      "-- Epoch 2\n",
      "Norm: 53.68, NNZs: 1064, Bias: 0.172020, T: 72, Avg. loss: 0.673827\n",
      "Norm: 157.41, NNZs: 1002, Bias: 0.412086, T: 72, Avg. loss: 1.255571\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "Norm: 100.86, NNZs: 995, Bias: 0.288542, T: 144, Avg. loss: 0.520680\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 68.54, NNZs: 972, Bias: 0.183422, T: 144, Avg. loss: 0.671354\n",
      "Norm: 78.17, NNZs: 972, Bias: 0.183118, T: 72, Avg. loss: 1.342639\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 11.09, NNZs: 995, Bias: 0.083385, T: 144, Avg. loss: 0.249223\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 152.71, NNZs: 1002, Bias: 0.412165, T: 144, Avg. loss: 0.627790\n",
      "Norm: 93.22, NNZs: 1037, Bias: 0.238062, T: 72, Avg. loss: 1.530751\n",
      "Norm: 113.74, NNZs: 1054, Bias: 0.261855, T: 72, Avg. loss: 1.466952\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 85.81, NNZs: 1037, Bias: 0.238125, T: 144, Avg. loss: 0.765539\n",
      "-- Epoch 2\n",
      "Norm: 104.68, NNZs: 1054, Bias: 0.261996, T: 144, Avg. loss: 0.733488\n",
      "Norm: 44.01, NNZs: 1064, Bias: 0.172067, T: 144, Avg. loss: 0.337202\n",
      "Norm: 15.39, NNZs: 1037, Bias: 0.091826, T: 72, Avg. loss: 0.395684\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 9.87, NNZs: 1037, Bias: 0.090749, T: 144, Avg. loss: 0.202758\n",
      "-- Epoch 1\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 81.85, NNZs: 1021, Bias: 0.203099, T: 72, Avg. loss: 1.236383\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 195.12, NNZs: 1064, Bias: 0.352934, T: 72, Avg. loss: 1.805567\n",
      "Total training time: 0.02 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 102.28, NNZs: 972, Bias: 0.224798, T: 72, Avg. loss: 1.603760\n",
      "Norm: 5.46, NNZs: 1021, Bias: 0.044797, T: 72, Avg. loss: 0.286501\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 195.60, NNZs: 1052, Bias: 0.377724, T: 72, Avg. loss: 1.841117\n",
      "-- Epoch 2\n",
      "Norm: 58.25, NNZs: 1054, Bias: 0.171235, T: 72, Avg. loss: 1.018214\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 71.76, NNZs: 1021, Bias: 0.203530, T: 144, Avg. loss: 0.618241\n",
      "Norm: 140.54, NNZs: 1002, Bias: 0.204997, T: 72, Avg. loss: 1.935080\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "Norm: 3.64, NNZs: 1021, Bias: 0.046389, T: 144, Avg. loss: 0.169180\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 191.66, NNZs: 1064, Bias: 0.353164, T: 144, Avg. loss: 0.902794\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "Total training time: 0.03 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 189.75, NNZs: 1052, Bias: 0.377837, T: 144, Avg. loss: 0.920565\n",
      "-- Epoch 2\n",
      "Norm: 133.59, NNZs: 1002, Bias: 0.205259, T: 144, Avg. loss: 0.967558\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Norm: 210.33, NNZs: 1054, Bias: 0.570878, T: 72, Avg. loss: 2.851636\n",
      "-- Epoch 1\n",
      "Norm: 47.74, NNZs: 1054, Bias: 0.171210, T: 144, Avg. loss: 0.509202\n",
      "Norm: 268.81, NNZs: 1086, Bias: 0.622674, T: 72, Avg. loss: 1.903459\n",
      "Total training time: 0.02 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "Norm: 206.59, NNZs: 1054, Bias: 0.570960, T: 144, Avg. loss: 1.425822\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 94.14, NNZs: 972, Bias: 0.225284, T: 144, Avg. loss: 0.801921\n",
      "-- Epoch 1\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 229.82, NNZs: 972, Bias: 0.690805, T: 72, Avg. loss: 2.744360\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 329.69, NNZs: 995, Bias: 0.675739, T: 72, Avg. loss: 4.488274\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.03 seconds.\n",
      "Norm: 213.57, NNZs: 972, Bias: 0.335758, T: 72, Avg. loss: 1.655123\n",
      "Norm: 159.46, NNZs: 1064, Bias: 0.234567, T: 72, Avg. loss: 2.067649\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 305.24, NNZs: 1086, Bias: 0.621909, T: 144, Avg. loss: 1.930172\n",
      "Norm: 209.89, NNZs: 972, Bias: 0.346630, T: 144, Avg. loss: 0.828071\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Norm: 154.70, NNZs: 1064, Bias: 0.235205, T: 144, Avg. loss: 1.033860\n",
      "Norm: 105.34, NNZs: 1021, Bias: 0.236276, T: 72, Avg. loss: 1.468113\n",
      "Norm: 227.42, NNZs: 972, Bias: 0.690813, T: 144, Avg. loss: 1.372180\n",
      "Norm: 108.18, NNZs: 1052, Bias: 0.286140, T: 72, Avg. loss: 1.067678\n",
      "-- Epoch 1\n",
      "Norm: 328.52, NNZs: 995, Bias: 0.676066, T: 144, Avg. loss: 2.244145\n",
      "Total training time: 0.02 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 174.18, NNZs: 1021, Bias: 0.424864, T: 72, Avg. loss: 2.384393\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 264.08, NNZs: 954, Bias: 0.714454, T: 72, Avg. loss: 1.761535\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 268.08, NNZs: 1021, Bias: 0.509935, T: 72, Avg. loss: 2.467283\n",
      "Norm: 102.83, NNZs: 1052, Bias: 0.286252, T: 144, Avg. loss: 0.533848\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 167.71, NNZs: 1054, Bias: 0.459966, T: 72, Avg. loss: 2.241678\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 96.95, NNZs: 1021, Bias: 0.236361, T: 144, Avg. loss: 0.734064\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 265.28, NNZs: 1021, Bias: 0.510156, T: 144, Avg. loss: 1.233650\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 162.70, NNZs: 1054, Bias: 0.460440, T: 144, Avg. loss: 1.120865\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 307.69, NNZs: 954, Bias: 0.521986, T: 144, Avg. loss: 1.826011\n",
      "Norm: 449.46, NNZs: 1037, Bias: 1.103138, T: 72, Avg. loss: 4.788986\n",
      "Norm: 139.61, NNZs: 1064, Bias: 0.424683, T: 72, Avg. loss: 1.471701\n",
      "-- Epoch 2\n",
      "Norm: 244.54, NNZs: 1086, Bias: 0.626306, T: 72, Avg. loss: 3.371984\n",
      "Norm: 88.33, NNZs: 1086, Bias: 0.161538, T: 72, Avg. loss: 1.026717\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 186.80, NNZs: 972, Bias: 0.449515, T: 72, Avg. loss: 1.781513\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 171.08, NNZs: 1021, Bias: 0.424972, T: 144, Avg. loss: 1.192202\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 132.71, NNZs: 1064, Bias: 0.424670, T: 144, Avg. loss: 0.735863\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 241.98, NNZs: 1086, Bias: 0.626315, T: 144, Avg. loss: 1.685993\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 447.86, NNZs: 1037, Bias: 1.103138, T: 144, Avg. loss: 2.394493\n",
      "Norm: 289.49, NNZs: 995, Bias: 0.628958, T: 72, Avg. loss: 4.281671\n",
      "Norm: 106.27, NNZs: 1054, Bias: 0.225458, T: 72, Avg. loss: 0.979867\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 181.22, NNZs: 972, Bias: 0.449591, T: 144, Avg. loss: 0.890761\n",
      "Norm: 89.86, NNZs: 1086, Bias: 0.241348, T: 144, Avg. loss: 0.795624\n",
      "Total training time: 0.02 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 250.61, NNZs: 954, Bias: 0.492160, T: 72, Avg. loss: 2.749241\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 471.98, NNZs: 994, Bias: 1.031812, T: 72, Avg. loss: 5.410546\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 101.02, NNZs: 1054, Bias: 0.226146, T: 144, Avg. loss: 0.489981\n",
      "-- Epoch 2\n",
      "Norm: 247.99, NNZs: 954, Bias: 0.492160, T: 144, Avg. loss: 1.374620\n",
      "Norm: 201.71, NNZs: 1086, Bias: 0.346111, T: 72, Avg. loss: 2.750851\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 471.00, NNZs: 1002, Bias: 1.031812, T: 144, Avg. loss: 2.705273\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 84.01, NNZs: 954, Bias: 0.228252, T: 72, Avg. loss: 0.705991\n",
      "Norm: 138.14, NNZs: 972, Bias: 0.202785, T: 72, Avg. loss: 1.693227\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Norm: 287.72, NNZs: 995, Bias: 0.628961, T: 144, Avg. loss: 2.140836\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 236.45, NNZs: 995, Bias: 0.538592, T: 72, Avg. loss: 3.460676\n",
      "-- Epoch 2\n",
      "Norm: 77.32, NNZs: 954, Bias: 0.228194, T: 144, Avg. loss: 0.353021\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 131.31, NNZs: 972, Bias: 0.203328, T: 144, Avg. loss: 0.846651\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 161.45, NNZs: 1021, Bias: 0.398865, T: 72, Avg. loss: 2.059159\n",
      "Norm: 209.60, NNZs: 1086, Bias: 0.467537, T: 144, Avg. loss: 1.386158\n",
      "-- Epoch 1\n",
      "Norm: 295.64, NNZs: 1037, Bias: 0.720810, T: 72, Avg. loss: 3.566600\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Norm: 163.92, NNZs: 1021, Bias: 0.371122, T: 72, Avg. loss: 1.811633\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 233.98, NNZs: 995, Bias: 0.538531, T: 144, Avg. loss: 1.730341\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 548.00, NNZs: 979, Bias: 1.074720, T: 72, Avg. loss: 4.756318\n",
      "-- Epoch 2\n",
      "Norm: 399.90, NNZs: 1046, Bias: 0.689029, T: 72, Avg. loss: 5.072071\n",
      "Norm: 156.63, NNZs: 1021, Bias: 0.398867, T: 144, Avg. loss: 1.029579\n",
      "Total training time: 0.02 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 293.83, NNZs: 1037, Bias: 0.720766, T: 144, Avg. loss: 1.783307\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Norm: 399.07, NNZs: 1052, Bias: 0.689507, T: 144, Avg. loss: 2.536046\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 209.10, NNZs: 1037, Bias: 0.421735, T: 72, Avg. loss: 1.928702\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 155.81, NNZs: 1021, Bias: 0.371133, T: 144, Avg. loss: 0.905817\n",
      "Norm: 547.34, NNZs: 979, Bias: 1.074720, T: 144, Avg. loss: 2.378159\n",
      "Norm: 183.94, NNZs: 1086, Bias: 0.375879, T: 72, Avg. loss: 2.303058\n",
      "Total training time: 0.02 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 192.92, NNZs: 954, Bias: 0.297932, T: 72, Avg. loss: 2.550949\n",
      "-- Epoch 1\n",
      "Norm: 465.39, NNZs: 1060, Bias: 0.750771, T: 72, Avg. loss: 4.853129\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 367.03, NNZs: 1002, Bias: 0.690753, T: 72, Avg. loss: 4.853624\n",
      "Norm: 178.44, NNZs: 1086, Bias: 0.375994, T: 144, Avg. loss: 1.151535\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Norm: 189.50, NNZs: 954, Bias: 0.298455, T: 144, Avg. loss: 1.275498\n",
      "Norm: 164.30, NNZs: 954, Bias: 0.398739, T: 72, Avg. loss: 2.249661\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 206.92, NNZs: 1037, Bias: 0.421760, T: 144, Avg. loss: 0.964352\n",
      "Norm: 484.20, NNZs: 1048, Bias: 0.960726, T: 72, Avg. loss: 7.122934\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 120.36, NNZs: 1086, Bias: 0.163817, T: 72, Avg. loss: 1.509890\n",
      "Norm: 497.79, NNZs: 1064, Bias: 1.075579, T: 144, Avg. loss: 2.473498\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 395.59, NNZs: 1002, Bias: 0.947532, T: 144, Avg. loss: 2.447390\n",
      "Norm: 199.60, NNZs: 995, Bias: 0.414633, T: 72, Avg. loss: 2.093526\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Norm: 127.07, NNZs: 1086, Bias: 0.261042, T: 144, Avg. loss: 0.779538\n",
      "Norm: 290.22, NNZs: 1002, Bias: 0.785382, T: 72, Avg. loss: 2.971482\n",
      "Norm: 483.62, NNZs: 1050, Bias: 0.960726, T: 144, Avg. loss: 3.561467\n",
      "Norm: 159.39, NNZs: 954, Bias: 0.398854, T: 144, Avg. loss: 1.124837\n",
      "Norm: 407.23, NNZs: 1052, Bias: 1.056637, T: 72, Avg. loss: 5.910325\n",
      "Total training time: 0.02 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 135.20, NNZs: 954, Bias: 0.419823, T: 72, Avg. loss: 1.787536\n",
      "-- Epoch 2\n",
      "Norm: 289.88, NNZs: 1052, Bias: 0.768590, T: 72, Avg. loss: 2.488784\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "Norm: 288.45, NNZs: 1002, Bias: 0.785362, T: 144, Avg. loss: 1.485742\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 196.06, NNZs: 995, Bias: 0.414857, T: 144, Avg. loss: 1.046773\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 406.39, NNZs: 1052, Bias: 1.060370, T: 144, Avg. loss: 2.955243\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 518.59, NNZs: 1054, Bias: 0.902585, T: 72, Avg. loss: 3.035833\n",
      "-- Epoch 2\n",
      "Norm: 288.84, NNZs: 1052, Bias: 0.768591, T: 144, Avg. loss: 1.244392\n",
      "Norm: 210.51, NNZs: 1037, Bias: 0.530933, T: 72, Avg. loss: 2.810021\n",
      "Total training time: 0.02 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 169.95, NNZs: 995, Bias: 0.439711, T: 72, Avg. loss: 2.320326\n",
      "Norm: 128.51, NNZs: 954, Bias: 0.419944, T: 144, Avg. loss: 0.893779\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 206.77, NNZs: 1037, Bias: 0.530937, T: 144, Avg. loss: 1.405011\n",
      "Norm: 270.77, NNZs: 1052, Bias: 0.499543, T: 72, Avg. loss: 3.147106\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 366.54, NNZs: 1062, Bias: 0.931393, T: 72, Avg. loss: 4.245653\n",
      "Total training time: 0.02 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 436.71, NNZs: 970, Bias: 0.553130, T: 72, Avg. loss: 5.601010\n",
      "Norm: 365.23, NNZs: 1064, Bias: 0.931393, T: 144, Avg. loss: 2.122826\n",
      "Norm: 286.19, NNZs: 1002, Bias: 0.548204, T: 72, Avg. loss: 3.307299\n",
      "Norm: 168.36, NNZs: 995, Bias: 0.502668, T: 144, Avg. loss: 1.164941\n",
      "Norm: 517.96, NNZs: 1054, Bias: 0.902585, T: 144, Avg. loss: 1.517916\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Norm: 273.59, NNZs: 1052, Bias: 0.584154, T: 144, Avg. loss: 1.576846\n",
      "Norm: 283.20, NNZs: 1002, Bias: 0.548204, T: 144, Avg. loss: 1.653650\n",
      "-- Epoch 2\n",
      "Total training time: 0.02 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 147.95, NNZs: 995, Bias: 0.407470, T: 72, Avg. loss: 2.163440\n",
      "Norm: 341.58, NNZs: 1052, Bias: 0.537742, T: 72, Avg. loss: 4.929674\n",
      "-- Epoch 1\n",
      "Norm: 435.81, NNZs: 970, Bias: 0.553137, T: 144, Avg. loss: 2.800505\n",
      "-- Epoch 1\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Norm: 175.92, NNZs: 1037, Bias: 0.396084, T: 72, Avg. loss: 1.885432\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 280.44, NNZs: 1064, Bias: 0.535230, T: 72, Avg. loss: 4.587865\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 513.19, NNZs: 1042, Bias: 1.005082, T: 72, Avg. loss: 3.950357\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 340.37, NNZs: 1054, Bias: 0.540436, T: 144, Avg. loss: 2.464907\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 217.22, NNZs: 1052, Bias: 0.369798, T: 72, Avg. loss: 1.341188\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 531.77, NNZs: 1048, Bias: 0.614675, T: 144, Avg. loss: 3.312841\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 140.63, NNZs: 995, Bias: 0.407478, T: 144, Avg. loss: 1.081720\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Norm: 393.68, NNZs: 1019, Bias: 0.681982, T: 72, Avg. loss: 2.890269\n",
      "Norm: 278.73, NNZs: 1064, Bias: 0.535244, T: 144, Avg. loss: 2.293933\n",
      "-- Epoch 1\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Norm: 526.02, NNZs: 960, Bias: 0.942242, T: 72, Avg. loss: 5.730823\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 525.39, NNZs: 960, Bias: 0.942242, T: 144, Avg. loss: 2.865411\n",
      "-- Epoch 2\n",
      "Norm: 353.70, NNZs: 972, Bias: 0.745076, T: 72, Avg. loss: 5.507072\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 170.66, NNZs: 1037, Bias: 0.396086, T: 144, Avg. loss: 0.942716\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 214.94, NNZs: 1052, Bias: 0.369798, T: 144, Avg. loss: 0.670594\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 135.84, NNZs: 1037, Bias: 0.246040, T: 72, Avg. loss: 1.537255\n",
      "Norm: 451.76, NNZs: 1021, Bias: 0.681787, T: 144, Avg. loss: 2.439597\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 291.98, NNZs: 1054, Bias: 0.703234, T: 72, Avg. loss: 3.892656\n",
      "-- Epoch 1\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 188.18, NNZs: 1002, Bias: 0.532089, T: 72, Avg. loss: 2.079351\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 129.13, NNZs: 1037, Bias: 0.246165, T: 144, Avg. loss: 0.768636\n",
      "-- Epoch 2\n",
      "Norm: 290.20, NNZs: 1054, Bias: 0.703315, T: 144, Avg. loss: 1.946331\n",
      "Norm: 184.84, NNZs: 1002, Bias: 0.532075, T: 144, Avg. loss: 1.039676\n",
      "Norm: 515.77, NNZs: 1007, Bias: 0.896219, T: 72, Avg. loss: 4.535733\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 352.44, NNZs: 972, Bias: 0.745077, T: 144, Avg. loss: 2.753536\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 255.94, NNZs: 1064, Bias: 0.819923, T: 72, Avg. loss: 2.569255\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 400.56, NNZs: 1086, Bias: 0.500366, T: 72, Avg. loss: 4.534726\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 399.73, NNZs: 1086, Bias: 0.500366, T: 144, Avg. loss: 2.267363\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 253.27, NNZs: 1064, Bias: 0.819927, T: 144, Avg. loss: 1.284628\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 561.63, NNZs: 1007, Bias: 1.286423, T: 144, Avg. loss: 2.410247\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 302.62, NNZs: 972, Bias: 0.966664, T: 72, Avg. loss: 3.380047\n",
      "Norm: 429.67, NNZs: 952, Bias: 0.640369, T: 72, Avg. loss: 4.293028\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 539.50, NNZs: 1052, Bias: 1.282716, T: 72, Avg. loss: 3.967854\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 198.20, NNZs: 1052, Bias: 0.416035, T: 72, Avg. loss: 2.612355\n",
      "-- Epoch 2\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 300.77, NNZs: 972, Bias: 0.966664, T: 144, Avg. loss: 1.690023\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "Norm: 428.78, NNZs: 952, Bias: 0.640376, T: 144, Avg. loss: 2.146514\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 347.26, NNZs: 1021, Bias: 0.879617, T: 72, Avg. loss: 3.977206\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 388.48, NNZs: 988, Bias: 0.769589, T: 72, Avg. loss: 4.566828\n",
      "Norm: 194.68, NNZs: 1052, Bias: 0.416035, T: 144, Avg. loss: 1.306177\n",
      "Norm: 240.01, NNZs: 1054, Bias: 0.539676, T: 72, Avg. loss: 1.908861\n",
      "-- Epoch 2\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 346.03, NNZs: 1021, Bias: 0.879644, T: 144, Avg. loss: 1.988604\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 539.12, NNZs: 1052, Bias: 1.282716, T: 144, Avg. loss: 1.983927\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Norm: 584.46, NNZs: 1086, Bias: 0.921671, T: 72, Avg. loss: 6.225054\n",
      "Norm: 262.36, NNZs: 1021, Bias: 0.604915, T: 72, Avg. loss: 3.294882\n",
      "Norm: 237.51, NNZs: 1054, Bias: 0.539676, T: 144, Avg. loss: 0.954431\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 310.91, NNZs: 1086, Bias: 0.701855, T: 72, Avg. loss: 3.335586\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "Norm: 624.13, NNZs: 1086, Bias: 1.307950, T: 144, Avg. loss: 3.144510\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 260.75, NNZs: 1021, Bias: 0.604917, T: 144, Avg. loss: 1.647441\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 782.20, NNZs: 948, Bias: 1.228967, T: 72, Avg. loss: 8.058127\n",
      "Total training time: 0.02 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Norm: 430.20, NNZs: 988, Bias: 1.094634, T: 144, Avg. loss: 2.390935\n",
      "Norm: 844.28, NNZs: 948, Bias: 1.791039, T: 144, Avg. loss: 4.188436\n",
      "Norm: 537.08, NNZs: 940, Bias: 0.722439, T: 72, Avg. loss: 5.769278\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 368.47, NNZs: 1086, Bias: 0.706645, T: 144, Avg. loss: 1.814797\n",
      "-- Epoch 2\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 580.74, NNZs: 940, Bias: 1.112085, T: 144, Avg. loss: 2.932634\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 533.82, NNZs: 976, Bias: 1.303981, T: 72, Avg. loss: 5.523725\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 468.37, NNZs: 1035, Bias: 0.887489, T: 72, Avg. loss: 5.924173\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Norm: 527.10, NNZs: 1044, Bias: 1.363759, T: 72, Avg. loss: 3.608015\n",
      "Norm: 533.18, NNZs: 976, Bias: 1.303981, T: 144, Avg. loss: 2.761862\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 467.40, NNZs: 1035, Bias: 0.887489, T: 144, Avg. loss: 2.962086\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 511.24, NNZs: 1017, Bias: 1.357581, T: 72, Avg. loss: 7.140034\n",
      "-- Epoch 2\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Norm: 635.44, NNZs: 1044, Bias: 0.895367, T: 144, Avg. loss: 3.991801\n",
      "Norm: 297.25, NNZs: 954, Bias: 0.661143, T: 72, Avg. loss: 1.350918\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 673.15, NNZs: 999, Bias: 1.351331, T: 72, Avg. loss: 8.355792\n",
      "Norm: 510.62, NNZs: 1031, Bias: 1.357581, T: 144, Avg. loss: 3.570017\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 745.03, NNZs: 999, Bias: 1.913535, T: 144, Avg. loss: 4.284690\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 296.19, NNZs: 954, Bias: 0.661143, T: 144, Avg. loss: 0.675459\n",
      "Norm: 535.69, NNZs: 962, Bias: 1.451400, T: 72, Avg. loss: 4.252626\n",
      "-- Epoch 1\n",
      "Norm: 531.40, NNZs: 971, Bias: 0.513721, T: 72, Avg. loss: 4.634511\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 535.35, NNZs: 971, Bias: 0.689049, T: 144, Avg. loss: 2.320512\n",
      "-- Epoch 2\n",
      "Norm: 535.32, NNZs: 964, Bias: 1.451360, T: 144, Avg. loss: 2.126314\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 771.18, NNZs: 1064, Bias: 1.305503, T: 72, Avg. loss: 9.719565\n",
      "Total training time: 0.01 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 770.87, NNZs: 1064, Bias: 1.305504, T: 144, Avg. loss: 4.859783\n",
      "-- Epoch 1\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 619.12, NNZs: 1036, Bias: 1.375673, T: 72, Avg. loss: 8.772209\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 618.69, NNZs: 1048, Bias: 1.375673, T: 144, Avg. loss: 4.386105\n",
      "-- Epoch 1\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 548.29, NNZs: 1007, Bias: 1.057833, T: 72, Avg. loss: 9.165896\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 547.90, NNZs: 1015, Bias: 1.058269, T: 144, Avg. loss: 4.582954\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 628.11, NNZs: 948, Bias: 1.078010, T: 72, Avg. loss: 7.469473\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 627.86, NNZs: 948, Bias: 1.078006, T: 144, Avg. loss: 3.734736\n",
      "-- Epoch 1\n",
      "Norm: 747.89, NNZs: 1068, Bias: 1.459804, T: 72, Avg. loss: 6.367349\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 747.36, NNZs: 1068, Bias: 1.459804, T: 144, Avg. loss: 3.183675\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 686.80, NNZs: 965, Bias: 1.064155, T: 72, Avg. loss: 5.942258\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 826.25, NNZs: 965, Bias: 2.187395, T: 144, Avg. loss: 3.331145\n",
      "Norm: 513.89, NNZs: 950, Bias: 1.017397, T: 72, Avg. loss: 4.968227\n",
      "Total training time: 0.00 seconds.\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 630.92, NNZs: 1015, Bias: 1.378195, T: 72, Avg. loss: 5.899688\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 630.67, NNZs: 1015, Bias: 1.378195, T: 144, Avg. loss: 2.949844\n",
      "-- Epoch 2\n",
      "Total training time: 0.00 seconds.\n",
      "Norm: 513.54, NNZs: 950, Bias: 1.017397, T: 144, Avg. loss: 2.484113\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 670.81, NNZs: 976, Bias: 2.271976, T: 72, Avg. loss: 8.352047\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 690.03, NNZs: 976, Bias: 1.849343, T: 144, Avg. loss: 4.192186\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 512.19, NNZs: 1019, Bias: 0.935007, T: 72, Avg. loss: 6.837827\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 511.83, NNZs: 1019, Bias: 0.935007, T: 144, Avg. loss: 3.418914\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 755.39, NNZs: 969, Bias: 1.514620, T: 72, Avg. loss: 9.587336\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 815.39, NNZs: 973, Bias: 2.076579, T: 144, Avg. loss: 4.978294\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 788.79, NNZs: 1028, Bias: 2.067929, T: 72, Avg. loss: 10.030512\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 788.47, NNZs: 1030, Bias: 2.067929, T: 144, Avg. loss: 5.015256\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 794.25, NNZs: 1040, Bias: 1.659348, T: 72, Avg. loss: 7.960070\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 793.93, NNZs: 1040, Bias: 1.659348, T: 144, Avg. loss: 3.980035\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 734.23, NNZs: 1037, Bias: 1.793320, T: 72, Avg. loss: 9.101220\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 733.93, NNZs: 1037, Bias: 1.793320, T: 144, Avg. loss: 4.550610\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.52, NNZs: 1113, Bias: 0.028110, T: 80, Avg. loss: 0.281260\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.55, NNZs: 1113, Bias: 0.030008, T: 160, Avg. loss: 0.188754\n",
      "Total training time: 0.00 seconds.\n",
      "*** took 0.6981s\n",
      "\n",
      "Best score: 1.000\n",
      "Best parameters set:\n",
      "\tclassifier__alpha: 0.048329302385717518\n"
     ]
    }
   ],
   "source": [
    "# here we set n_iter=2 just for testing purposes\n",
    "job['n_iter'] = 2\n",
    "CV = crossValOptimize(job)\n",
    "CV.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Train Classifier\n",
    "Uses features and labels from `features_pickle_file` and `labels_pickle_file` to train the classifier using the parameters from `best_params_file`. The trained classifier is then saved to `classifier_filename`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier with the following parameters:\n",
      "    loss         = log\n",
      "    alpha        = 0.0483293024\n",
      "    n_iter       = 2\n",
      "    penalty      = l2\n",
      "    class_weight = None\n",
      "\n",
      "fitting classifier\n",
      "*** took 0.004971s\n"
     ]
    }
   ],
   "source": [
    "trainClassifier(job).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Classify the tweets\n",
    "Adds two tables `class_proba` and `retweet_class_proba` to the SQLite database with the result of the classification of each tweets and original retweeted status.\n",
    "\n",
    "*Optional parameters:*\n",
    "- `propa_table_name_suffix` : add a suffix to the two table names in order to compare different classifiers. Default is '' (empty string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading classifier.pickle\n",
      "Table retweet_class_proba already exists in database.\n",
      "Do you want to drop the table (irreversibly delete it) and replace it? (y/n)y\n",
      "0 over 0.0307\n",
      "** row : 0 to 9999\n",
      "\n",
      "total time : 6.389617919921875e-05\n",
      "getting tweets from retweeted_status\n",
      "updating retweet_class_proba\n",
      "finished\n",
      "*** took 0.04318s\n",
      "Table class_proba already exists in database.\n",
      "Do you want to drop the table (irreversibly delete it) and replace it? (y/n)y\n",
      "0 over 1.6071\n",
      "** row : 0 to 9999\n",
      "\n",
      "total time : 5.555152893066406e-05\n",
      "getting tweets from tweet\n",
      "updating class_proba\n",
      "1 over 1.6071\n",
      "** row : 10000 to 19999\n",
      "\n",
      "total time : 1.3122282028198242\n",
      "getting tweets from tweet\n",
      "updating class_proba\n",
      "finished\n",
      "*** took 2.142s\n"
     ]
    }
   ],
   "source": [
    "classifyTweets(job).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Analyze classification results\n",
    "`makeProbaDF` reads the classification results from the database and processes them to:\n",
    "- Replace the classification probability of retweets with the classification results of the original tweets.\n",
    "- Replace the classification probability of tweets having a hashtag of one of the two camps (and not of the other camp) with 0 (for camp1) or 1 (for camp2).\n",
    "- Discard tweets emanating from unoffical Twitter clients.\n",
    "\n",
    "The results are saved as a pandas dataframe in `df_proba_filename`.\n",
    "\n",
    "*Optional parameters:*\n",
    "- `use_official_clients` : whether you want to keep only tweets from official clients (`True`) or all tweets (`False`). Default is `True`.\n",
    "- `propa_table_name_suffix` can be changed to use the classification of different classifiers if it was used with `classifyTweets`.\n",
    "- `column_name_ht_group` is also used if it was changed to create a different training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "querying sql\n",
      "creating df proba\n",
      "creating df_proba_original_rt\n",
      "*** took 0.09746s\n",
      "creating df_proba_rt\n",
      "*** took 0.1656s\n",
      "creating df_proba_ht_pro_0\n",
      "*** took 0.1843s\n",
      "creating df_proba_ht_pro_1\n",
      "*** took 0.2174s\n",
      "saving corrected dataframe\n",
      "done\n",
      "*** took 0.2397s\n"
     ]
    }
   ],
   "source": [
    "makeProbaDF(job).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`analyzeProbaDF` reads `df_proba_filename` and returns the number of tweets and the number of users in each camp per day. The results are displayed and saved as pandas dataframes to `df_num_tweets_filename` and `df_num_users_filename`.\n",
    "\n",
    "*Optional parameters:*\n",
    "- `ncpu` : number of cores to use. Default is number of cores of the machine minus one.\n",
    "- `resampling_frequency` : frequency at which tweets are grouped. Default is `'D'`, i.e. daily. (see [here](http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases) for different possibilities.)\n",
    "- `threshold` : threshold for the classifier probability (threshold >= 0.5). Tweets with p > threshold are classified in camp2 and tweets with p < 1-threshold are classified in camp1. Default is 0.5.\n",
    "- `r_threshold` : threshold for the ratio of classified tweets needed to classify a user. Default is 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading df_proba.pickle\n",
      "threshold: 0.5\n",
      "r_threshold: 0.5\n",
      "computing stats\n",
      "finished\n",
      "1.0772144794464111\n",
      "Number of tweets per day\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_pro_0</th>\n",
       "      <th>n_pro_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-10-20</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-21</th>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-22</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-24</th>\n",
       "      <td>119</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-25</th>\n",
       "      <td>40</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-26</th>\n",
       "      <td>48</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-27</th>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-28</th>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-29</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-30</th>\n",
       "      <td>75</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-31</th>\n",
       "      <td>117</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-01</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-02</th>\n",
       "      <td>7</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-03</th>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-04</th>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-05</th>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-06</th>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-07</th>\n",
       "      <td>9</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-08</th>\n",
       "      <td>7</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-09</th>\n",
       "      <td>268</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-10</th>\n",
       "      <td>43</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-11</th>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-12</th>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-13</th>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-14</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-15</th>\n",
       "      <td>11</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-16</th>\n",
       "      <td>2</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-17</th>\n",
       "      <td>3</td>\n",
       "      <td>3423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-18</th>\n",
       "      <td>10</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-19</th>\n",
       "      <td>4</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-20</th>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-21</th>\n",
       "      <td>79</td>\n",
       "      <td>3797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-22</th>\n",
       "      <td>29</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-23</th>\n",
       "      <td>10</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-24</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-25</th>\n",
       "      <td>10</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-26</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-27</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-28</th>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-29</th>\n",
       "      <td>2</td>\n",
       "      <td>4168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-30</th>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-02</th>\n",
       "      <td>9</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-03</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-04</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-05</th>\n",
       "      <td>12</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-06</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-07</th>\n",
       "      <td>12</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-08</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-10</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-11</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-12</th>\n",
       "      <td>4</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-13</th>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            n_pro_0  n_pro_1\n",
       "2016-10-20        1        9\n",
       "2016-10-21        5       67\n",
       "2016-10-22       10       16\n",
       "2016-10-24      119       42\n",
       "2016-10-25       40       56\n",
       "2016-10-26       48       93\n",
       "2016-10-27        0       35\n",
       "2016-10-28       16       19\n",
       "2016-10-29        0       10\n",
       "2016-10-30       75       11\n",
       "2016-10-31      117       19\n",
       "2016-11-01        1       30\n",
       "2016-11-02        7       36\n",
       "2016-11-03        5       24\n",
       "2016-11-04        2       40\n",
       "2016-11-05       15        9\n",
       "2016-11-06       21        6\n",
       "2016-11-07        9       31\n",
       "2016-11-08        7       43\n",
       "2016-11-09      268      196\n",
       "2016-11-10       43      163\n",
       "2016-11-11        6       25\n",
       "2016-11-12        4       22\n",
       "2016-11-13       18        9\n",
       "2016-11-14        1       28\n",
       "2016-11-15       11       71\n",
       "2016-11-16        2      123\n",
       "2016-11-17        3     3423\n",
       "2016-11-18       10      447\n",
       "2016-11-19        4       59\n",
       "2016-11-20        2       60\n",
       "2016-11-21       79     3797\n",
       "2016-11-22       29      243\n",
       "2016-11-23       10       53\n",
       "2016-11-24        0       61\n",
       "2016-11-25       10       45\n",
       "2016-11-26        0       13\n",
       "2016-11-27        0       12\n",
       "2016-11-28       12       18\n",
       "2016-11-29        2     4168\n",
       "2016-11-30        2       94\n",
       "2016-12-01        5       41\n",
       "2016-12-02        9       34\n",
       "2016-12-03        3        6\n",
       "2016-12-04        5        8\n",
       "2016-12-05       12       38\n",
       "2016-12-06        3       24\n",
       "2016-12-07       12       50\n",
       "2016-12-08        7        6\n",
       "2016-12-10        0        3\n",
       "2016-12-11        0        7\n",
       "2016-12-12        4       44\n",
       "2016-12-13        8       18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users per day\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_pro_0</th>\n",
       "      <th>n_pro_1</th>\n",
       "      <th>null</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-10-20</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-21</th>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-22</th>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-24</th>\n",
       "      <td>79</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-25</th>\n",
       "      <td>34</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-26</th>\n",
       "      <td>34</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-27</th>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-28</th>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-29</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-30</th>\n",
       "      <td>65</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-31</th>\n",
       "      <td>84</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-01</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-02</th>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-03</th>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-04</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-05</th>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-06</th>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-07</th>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-08</th>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-09</th>\n",
       "      <td>132</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-10</th>\n",
       "      <td>34</td>\n",
       "      <td>111</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-11</th>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-12</th>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-13</th>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-14</th>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-15</th>\n",
       "      <td>9</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-16</th>\n",
       "      <td>1</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-17</th>\n",
       "      <td>3</td>\n",
       "      <td>3404</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-18</th>\n",
       "      <td>5</td>\n",
       "      <td>390</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-19</th>\n",
       "      <td>3</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-20</th>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-21</th>\n",
       "      <td>56</td>\n",
       "      <td>3782</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-22</th>\n",
       "      <td>25</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-23</th>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-24</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-25</th>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-26</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-27</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-28</th>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-29</th>\n",
       "      <td>2</td>\n",
       "      <td>4163</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-30</th>\n",
       "      <td>2</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-02</th>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-03</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-04</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-05</th>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-06</th>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-07</th>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-08</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-10</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-11</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-12</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-13</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            n_pro_0  n_pro_1  null\n",
       "2016-10-20        1        9     0\n",
       "2016-10-21        4       45     0\n",
       "2016-10-22       10       13     0\n",
       "2016-10-24       79       26     0\n",
       "2016-10-25       34       50     0\n",
       "2016-10-26       34       81     7\n",
       "2016-10-27        0       26     0\n",
       "2016-10-28       16       15     0\n",
       "2016-10-29        0        8     0\n",
       "2016-10-30       65       11     0\n",
       "2016-10-31       84       14     0\n",
       "2016-11-01        1       25     0\n",
       "2016-11-02        6       30     1\n",
       "2016-11-03        4       18     1\n",
       "2016-11-04        2       22     0\n",
       "2016-11-05       14        6     1\n",
       "2016-11-06       17        6     0\n",
       "2016-11-07        8       24     0\n",
       "2016-11-08        4       39     0\n",
       "2016-11-09      132      141     0\n",
       "2016-11-10       34      111     2\n",
       "2016-11-11        5       19     1\n",
       "2016-11-12        3       19     0\n",
       "2016-11-13       18        8     0\n",
       "2016-11-14        0       21     0\n",
       "2016-11-15        9       65     1\n",
       "2016-11-16        1      113     0\n",
       "2016-11-17        3     3404     0\n",
       "2016-11-18        5      390     2\n",
       "2016-11-19        3       57     0\n",
       "2016-11-20        2       60     0\n",
       "2016-11-21       56     3782     0\n",
       "2016-11-22       25      229     0\n",
       "2016-11-23        3       51     0\n",
       "2016-11-24        0       61     0\n",
       "2016-11-25        2       28     0\n",
       "2016-11-26        0       11     0\n",
       "2016-11-27        0       12     0\n",
       "2016-11-28        3       14     0\n",
       "2016-11-29        2     4163     0\n",
       "2016-11-30        2       83     0\n",
       "2016-12-01        5       32     0\n",
       "2016-12-02        7       28     0\n",
       "2016-12-03        3        6     0\n",
       "2016-12-04        4        8     0\n",
       "2016-12-05        3       29     0\n",
       "2016-12-06        2       21     1\n",
       "2016-12-07        4       40     0\n",
       "2016-12-08        1        6     0\n",
       "2016-12-10        0        3     0\n",
       "2016-12-11        0        7     0\n",
       "2016-12-12        2       42     1\n",
       "2016-12-13        7       13     0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyzeProbaDF(job).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_params_file': 'best_params.json',\n",
       " 'classifier_filename': 'classifier.pickle',\n",
       " 'df_num_tweets_filename': 'df_num_tweets.pickle',\n",
       " 'df_num_users_filename': 'df_num_users.pickle',\n",
       " 'df_proba_filename': 'df_proba.pickle',\n",
       " 'features_pickle_file': 'features.pickle',\n",
       " 'features_vect_file': 'features.mmap',\n",
       " 'graph_file': 'graph_file.graphml',\n",
       " 'htgs_lists': [['mortgage',\n",
       "   'rates',\n",
       "   'loan',\n",
       "   'loans',\n",
       "   'lenders',\n",
       "   'amortization',\n",
       "   'subprime',\n",
       "   'interest'],\n",
       "  ['etrade',\n",
       "   'tradeking',\n",
       "   'stock',\n",
       "   '401k',\n",
       "   'market',\n",
       "   'ameritrade',\n",
       "   'scottrade',\n",
       "   'finance',\n",
       "   'money']],\n",
       " 'initial_htgs_lists': [['mortgage'], ['etrade']],\n",
       " 'labels_mappers_file': 'labels_mappers.pickle',\n",
       " 'labels_pickle_file': 'labels.pickle',\n",
       " 'labels_vect_file': 'labels.mmap',\n",
       " 'n_iter': 2,\n",
       " 'propag_results_filename': 'propag_results.pickle',\n",
       " 'sqlite_db_filename': 'test.sqlite',\n",
       " 'tweet_archive_dirs': ['etrade']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print all job parameters\n",
    "job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
